import Foundation
import UIKit
import CoreImage
import Combine
import CoreML
import AVFoundation

// MARK: - Analysis State (Grouped for Performance)
struct AnalysisState: Equatable {
    var instantFeedback: [FeedbackItem] = []
    var isPerfect: Bool = false
    var perfectScore: Double = 0.0
    var categoryStatuses: [CategoryStatus] = []
    var completedFeedbacks: [CompletedFeedback] = []
    var gateEvaluation: GateEvaluation?
    var v15Feedback: String = ""
    var unifiedFeedback: UnifiedFeedback?
    var stabilityProgress: Float = 0.0 // ğŸ†• 0.0 ~ 1.0 (Temporal Lock)
    var environmentWarning: String?      // ğŸ†• í™˜ê²½ ê²½ê³  (ë„ˆë¬´ ì–´ë‘ì›€ ë“±)
    var currentShotDebugInfo: String?    // ğŸ†• í™”ë©´ í‘œì‹œìš© ìƒ·íƒ€ì… ì •ë³´ (Debug Mode)

    // ğŸ†• ì•ˆì •ì ì¸ í”¼ë“œë°± (ë™ì¼ í”¼ë“œë°±ì€ ì§„í–‰ë¥ ë§Œ ì—…ë°ì´íŠ¸)
    var activeFeedback: ActiveFeedback?

    // ğŸ†• ë‹¨ìˆœí™”ëœ ì‹¤ì‹œê°„ ê°€ì´ë“œ ê²°ê³¼
    var simpleGuide: SimpleGuideResult?
}

// MARK: - ì‹¤ì‹œê°„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡°
struct FrameAnalysis {
    let faceRect: CGRect?                           // ì–¼êµ´ ìœ„ì¹˜ (ì •ê·œí™”ëœ ì¢Œí‘œ)
    let bodyRect: CGRect?                           // ì „ì‹  ì¶”ì • ì˜ì—­
    let brightness: Float                           // í‰ê·  ë°ê¸°
    let tiltAngle: Float                            // ê¸°ìš¸ê¸° ê°ë„
    let faceYaw: Float?                             // ì–¼êµ´ ì¢Œìš° íšŒì „ (ì •ë©´=0)
    let facePitch: Float?                           // ì–¼êµ´ ìƒí•˜ ê°ë„
    let cameraAngle: CameraAngle                    // ì¹´ë©”ë¼ ê°ë„
    let poseKeypoints: [(point: CGPoint, confidence: Float)]?  // ì‹ ë¢°ë„ í¬í•¨ í‚¤í¬ì¸íŠ¸
    let compositionType: CompositionType?           // êµ¬ë„ íƒ€ì…
    // ğŸ—‘ï¸ VNFaceObservation ì œê±° (RTMPoseë¡œ ëŒ€ì²´)
    let gaze: GazeResult?                           // ğŸ†• ì‹œì„  ì¶”ì  ê²°ê³¼
    let depth: V15DepthResult?                      // ğŸ”¥ Depth Anything ML ê¸°ë°˜ ê¹Šì´ ì¶”ì •
    let aspectRatio: CameraAspectRatio              // ğŸ†• ì¹´ë©”ë¼ ë¹„ìœ¨
    let imagePadding: ImagePadding?                 // ğŸ†• ì—¬ë°± ì •ë³´
}

// ğŸ†• ì´ë¯¸ì§€ ì—¬ë°± ì •ë³´
struct ImagePadding {
    let top: CGFloat        // ìƒë‹¨ ì—¬ë°± (0.0 ~ 1.0)
    let bottom: CGFloat     // í•˜ë‹¨ ì—¬ë°±
    let left: CGFloat       // ì¢Œì¸¡ ì—¬ë°±
    let right: CGFloat      // ìš°ì¸¡ ì—¬ë°±

    var total: CGFloat {
        return top + bottom + left + right
    }

    var hasExcessivePadding: Bool {
        // ì–´ëŠ í•œ ìª½ì´ 15% ì´ìƒ ì—¬ë°±ì´ë©´ ê³¼ë„í•¨
        return top > 0.15 || bottom > 0.15 || left > 0.15 || right > 0.15
    }
}

// MARK: - ì‹¤ì‹œê°„ í”¼ë“œë°± ìƒì„±ê¸°
class RealtimeAnalyzer: ObservableObject {
    // MARK: - Published State
    @Published var state = AnalysisState()
    
    // ğŸ”¥ Detection Pipeline Integration
    private let pipeline = DetectionPipeline()
    private var pipelineCancellables = Set<AnyCancellable>()
    
    // ğŸ’¡ Wrapper properties for backward compatibility (read-only)
    var instantFeedback: [FeedbackItem] { state.instantFeedback }
    var isPerfect: Bool { state.isPerfect }
    var perfectScore: Double { state.perfectScore }
    var categoryStatuses: [CategoryStatus] { state.categoryStatuses }
    var completedFeedbacks: [CompletedFeedback] { state.completedFeedbacks }
    var gateEvaluation: GateEvaluation? { state.gateEvaluation }
    var v15Feedback: String { state.v15Feedback }
    var unifiedFeedback: UnifiedFeedback? { state.unifiedFeedback }
    var stabilityProgress: Float { state.stabilityProgress }

    var environmentWarning: String? { state.environmentWarning }
    var currentShotDebugInfo: String? { state.currentShotDebugInfo }
    var activeFeedback: ActiveFeedback? { state.activeFeedback }
    var simpleGuide: SimpleGuideResult? { state.simpleGuide }

    // ğŸ› ContentViewì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ã„±ë¡ internalë¡œ ë³€ê²½
    var referenceAnalysis: FrameAnalysis?
    var referenceFramingResult: PhotographyFramingResult?  // ğŸ†• ë ˆí¼ëŸ°ìŠ¤ ì‚¬ì§„í•™ í”„ë ˆì´ë° ë¶„ì„ ê²°ê³¼

    // ğŸ†• v1.5 ìºì‹œëœ ë ˆí¼ëŸ°ìŠ¤
    var cachedReference: CachedReference?

    private var lastAnalysisTime = Date()
    private let analysisInterval: TimeInterval = 0.05  // 50msë§ˆë‹¤ ë¶„ì„ - ë°˜ì‘ì†ë„ ê°œì„ 

    // ğŸ”¥ ë¶„ì„ ì „ìš© ë°±ê·¸ë¼ìš´ë“œ í (UI ë¸”ë¡œí‚¹ ë°©ì§€)
    private let analysisQueue = DispatchQueue(label: "com.tryangle.analysis", qos: .userInitiated)
    private var isAnalyzing = false  // ë¶„ì„ ì¤‘ë³µ ë°©ì§€ í”Œë˜ê·¸
    private var isPaused = false     // ì¼ì‹œ ì¤‘ì§€ í”Œë˜ê·¸ (íƒ­ ì „í™˜ ì‹œ)

    // íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ë¥¼ ìœ„í•œ ìƒíƒœ ì¶”ì 
    private var feedbackHistory: [String: Int] = [:]  // ì¹´í…Œê³ ë¦¬ë³„ ì—°ì† ê°ì§€ íšŸìˆ˜
    private let historyThreshold = 3  // ğŸ”„ 3ë²ˆ ì—°ì† ê°ì§€ë˜ì–´ì•¼ í‘œì‹œ (ì•½ 0.3ì´ˆ) - ë°˜ì‘ì†ë„ ê°œì„ 
    private var perfectFrameCount = 0  // ì™„ë²½í•œ í”„ë ˆì„ ì—°ì† íšŸìˆ˜
    private let perfectThreshold = 5  // ìœ ì§€ìš© (Temporal Lock ì´ì „ í•˜ìœ„ í˜¸í™˜)

    // ğŸ†• Phase 2: Temporal Lock (ì•ˆì •í™” íƒ€ì´ë¨¸)
    private enum GateStabilityState: Equatable {
        case idle
        case arming(startedAt: Date)
        case locked
    }
    private var stabilityState: GateStabilityState = .idle
    private let lockDuration: TimeInterval = 0.5  // 0.5ì´ˆ ìœ ì§€ ì‹œ ì„±ê³µ

    // ğŸ†• ê³ ì • í”¼ë“œë°± (í•œ ë²ˆ í‘œì‹œë˜ë©´ í•´ê²°ë  ë•Œê¹Œì§€ ìœ ì§€)
    private var stickyFeedbacks: [String: FeedbackItem] = [:]  // ì¹´í…Œê³ ë¦¬ë³„ ê³ ì • í”¼ë“œë°±

    // ğŸ†• ì´ì „ í”„ë ˆì„ì˜ í”¼ë“œë°± (ì™„ë£Œ ê°ì§€ìš©)
    private var previousFeedbackIds = Set<String>()
    // ğŸ†• ì™„ë£Œ ê°ì§€ë¥¼ ìœ„í•œ íˆìŠ¤í…Œë¦¬ì‹œìŠ¤
    private var disappearedFeedbackHistory: [String: Int] = [:]  // ì‚¬ë¼ì§„ í”¼ë“œë°±ì˜ ì—°ì† íšŸìˆ˜
    private let disappearedThreshold = 2  // 2ë²ˆ ì—°ì† ì‚¬ë¼ì ¸ì•¼ ì™„ë£Œë¡œ íŒë‹¨ - ë°˜ì‘ì†ë„ ê°œì„ 

    // ğŸ†• Phase 2: Adaptive Difficulty (ì¢Œì ˆ ê°ì§€)
    private var feedbackStartTimes: [String: Date] = [:]
    private var frustrationMultiplier: CGFloat = 1.0
    private let frustrationThreshold: TimeInterval = 5.0 // 5ì´ˆê°„ í•´ê²° ëª»í•˜ë©´ ë‚œì´ë„ ì™„í™”

    // ğŸ†• ê³ ì • í”¼ë“œë°± ì¹´í…Œê³ ë¦¬ (í¬ì¦ˆ ê´€ë ¨ì€ ê³„ì† í‘œì‹œ)
    // pose_missing_partsëŠ” ì´ì œ ë ˆí¼ëŸ°ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ì œëŒ€ë¡œ ê°ì§€ë˜ë¯€ë¡œ sticky ì²˜ë¦¬
    private let stickyCategories: Set<String> = [
        "pose_left_arm",
        "pose_right_arm",
        "pose_left_leg",
        "pose_right_leg",
        "pose_missing_parts"
    ]

    // ğŸ”¥ RTMPose ë¶„ì„ê¸° (ONNX Runtime with CoreML EP)
    private var poseMLAnalyzer: PoseMLAnalyzer!
    private let compositionAnalyzer = CompositionAnalyzer()
    private let cameraAngleDetector = CameraAngleDetector()
    
    // ğŸ†• Image Processing Context
    private let ciContext = CIContext(options: [.cacheIntermediates: false])
    
    private var cancellables = Set<AnyCancellable>()
    
    // MARK: - Subscription Setup
    func setupSubscription(framePublisher: AnyPublisher<CMSampleBuffer, Never>, cameraManager: CameraManager) {
        // ğŸ”¥ ì¤‘ë³µ êµ¬ë… ë°©ì§€: ê¸°ì¡´ êµ¬ë… ì·¨ì†Œ
        cancellables.removeAll()
        
        framePublisher
            .sink { [weak self] buffer in
                guard let self = self else { return }
                self.process(
                    buffer: buffer,
                    isFrontCamera: cameraManager.isFrontCamera,
                    currentAspectRatio: cameraManager.aspectRatio, // Note: Accessed on background thread?
                    zoomFactor: cameraManager.virtualZoom
                )
            }
            .store(in: &cancellables)
    }
    
    // Note: accessing cameraManager properties (published) from background sink might be racey if not thread safe.
    // However, CameraManager @Published props are updated on Main Thread.
    // Reading them from background thread is generally TSan unsafe but widely done.
    // Ideally, we should receive these values as a combined stream.
    // But for now, since they change rarely compared to frames, reading current value is acceptable risk or we can assume `process` usage.
    // Actually, `process` does `analysisQueue.async`.
    // So we are capturing `cameraManager` instance.
    // Better approach: combineLatest? 
    // Frame comes at 60fps. Changes in zoom/ratio are rare.
    // `cameraManager` is ObservableObject.
    // We can just read properties.
    
    // MARK: - Buffer Processing (Combine Bridge)
    func process(buffer: CMSampleBuffer, isFrontCamera: Bool, currentAspectRatio: CameraAspectRatio, zoomFactor: CGFloat) {
        // Drop frame if analyzing
        // guard !isAnalyzing else { return } // Pipeline handles dropping? Pipeline has isProcessing check.
        // We use pipeline.isProcessing internally. 
        
        // Throttling handled by Pipeline implicitly if we don't await? 
        // Actually pipeline.process is async fire-and-forget but checks isProcessing.
        
        guard !isPaused else { return }
        
        self.currentZoomFactor = zoomFactor
        
        // Extract brightness
        var brightness: Double?
        if let pixelBuffer = CMSampleBufferGetImageBuffer(buffer) {
             let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
             if let exif = ciImage.properties["{Exif}"] as? [String: Any] {
                 brightness = exif["BrightnessValue"] as? Double
             }
        }
        
        // Create FrameInput
        // Note: Creating UIImage from buffer is heavy. Pipeline expects FrameInput.
        // If pipeline can take buffer, better. But FrameInput takes UIImage.
        // conversion logic:
        
        analysisQueue.async { [weak self] in
            guard let self = self else { return }
            
            guard let pixelBuffer = CMSampleBufferGetImageBuffer(buffer) else { return }
            let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
            let context = self.ciContext
            guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else { return }
            
            let image = UIImage(cgImage: cgImage, scale: 1.0, orientation: .up)
            
            var metadata: [String: Any] = [:]
            if let b = brightness {
                metadata["BrightnessValue"] = b
            }
            
            let input = FrameInput(
                image: image,
                timestamp: Date().timeIntervalSince1970,
                cameraPosition: isFrontCamera ? .front : .back,
                orientation: .up,
                metadata: metadata
            )
            
            self.pipeline.process(input: input)
        }
    }
    
    private func resetAnalyzingFlag() {
        DispatchQueue.main.async { self.isAnalyzing = false }
    }
    private let gazeTracker = GazeTracker()
    private let depthAnything = DepthAnythingCoreML.shared  // ğŸ”¥ ì‹±ê¸€í†¤ ì‚¬ìš© (ë©”ëª¨ë¦¬ ìµœì í™”)
    private let poseComparator = AdaptivePoseComparator()
    // framingAnalyzer ì œê±°ë¨ - Legacy í´ë”ë¡œ ì´ë™ (2025-12-29)
    private let photographyFramingAnalyzer = PhotographyFramingAnalyzer()  // ì‚¬ì§„í•™ ê¸°ë°˜ í”„ë ˆì´ë° ë¶„ì„ê¸°

    // ğŸ†• v1.5 í†µí•© Gate System (5ë‹¨ê³„)
    private let gateSystem = GateSystem.shared
    private let marginAnalyzer = MarginAnalyzer()
    private let personDetector = PersonDetector()  // ì •ë°€ BBox (30í”„ë ˆì„ë§ˆë‹¤) - YOLOX ì¬ì‚¬ìš©
    private let focalLengthEstimator = FocalLengthEstimator.shared  // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬

    // ğŸ†• ë‹¨ìˆœí™”ëœ ì‹¤ì‹œê°„ ê°€ì´ë“œ ì‹œìŠ¤í…œ (GateSystem ëŒ€ì²´ìš©)
    private let simpleRealTimeGuide = SimpleRealTimeGuide.shared

    // ğŸ†• v1.5 í”„ë ˆì„ ì¹´ìš´í„° (Level ì²˜ë¦¬ìš©)
    private var frameCount = 0
    private var lastYOLOXBBox: CGRect?           // ğŸ†• YOLOX BBox ìºì‹œ (ë§¤ í”„ë ˆì„ ê°±ì‹ )
    private var lastPoseKeypoints: [(point: CGPoint, confidence: Float)]?  // ğŸ†• RTMPose í‚¤í¬ì¸íŠ¸ ìºì‹œ
    private var lastPoseResult: PoseAnalysisResult?  // ğŸ†• RTMPose ê²°ê³¼ ìºì‹œ
    private var lastCompressionIndex: CGFloat?  // ë§ˆì§€ë§‰ ì••ì¶•ê° ìºì‹œ
    private var lastDepthResult: V15DepthResult?   // ğŸ”¥ Depth Anything ê²°ê³¼ ìºì‹œ

    // ğŸ†• RTMPose í˜¸ì¶œ ì£¼ê¸° (ë§¤ í”„ë ˆì„ - iPhone 16 Pro ìµœì í™”)
    // A18 Pro Neural Engineì´ ì¶©ë¶„íˆ ì²˜ë¦¬ ê°€ëŠ¥, ë°œì—´ ì‹œ ë‹¤ì‹œ 2~3ìœ¼ë¡œ ì¡°ì •
    private let rtmPoseInterval: Int = 1

    // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬ ê´€ë ¨
    private var referenceImageData: Data?       // ë ˆí¼ëŸ°ìŠ¤ EXIF ì¶”ì¶œìš©
    private var referenceDepthMap: MLMultiArray?  // ë ˆí¼ëŸ°ìŠ¤ ëìŠ¤ë§µ (EXIF ì—†ì„ ë•Œ fallback)
    private var referenceFocalLength: FocalLengthInfo?  // ìºì‹œëœ ë ˆí¼ëŸ°ìŠ¤ ì´ˆì ê±°ë¦¬
    var currentZoomFactor: CGFloat = 1.0        // í˜„ì¬ ì¤Œ ë°°ìœ¨ (CameraManagerì—ì„œ ì—…ë°ì´íŠ¸)

    // ğŸ†• ëª©í‘œ ì¤Œ ë°°ìœ¨ (ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ì‹œ í•œ ë²ˆë§Œ ê³„ì‚°, ì´í›„ ê³ ì •)
    private var targetZoomFactor: CGFloat?      // ì˜ˆ: 2.4x - nilì´ë©´ ì¤Œ ì²´í¬ ì•ˆí•¨

    // ğŸ”¥ ì„±ëŠ¥ ìµœì í™”
    private let thermalManager = ThermalStateManager()
    private let frameSkipper = AdaptiveFrameSkipper()
    private var lastPerformanceLog = Date()

    // ğŸ†• ì´ˆê¸°í™”
    init() {
        // Setup Pipeline
        self.setupPipeline()
        
        // ... (Keep existing bg init for Reference Analyzer if needed)

        // print("ğŸ¬ğŸ¬ğŸ¬ RealtimeAnalyzer init() í˜¸ì¶œë¨ ğŸ¬ğŸ¬ğŸ¬")

        // ğŸ”¥ PoseMLAnalyzerë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¯¸ë¦¬ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ 17ì´ˆ ì§€ì—° ë°©ì§€)
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            // print("ğŸ”¥ RealtimeAnalyzer: PoseMLAnalyzer ë°±ê·¸ë¼ìš´ë“œ ì´ˆê¸°í™” ì‹œì‘")
            let startTime = CACurrentMediaTime()
            let analyzer = PoseMLAnalyzer()
            let loadTime = (CACurrentMediaTime() - startTime) * 1000
            // print("âœ… RealtimeAnalyzer: PoseMLAnalyzer ì´ˆê¸°í™” ì™„ë£Œ (\(String(format: "%.0f", loadTime))ms)")

            DispatchQueue.main.async {
                self?.poseMLAnalyzer = analyzer

                // ğŸ”¥ PersonDetectorì— RTMPoseRunner ì—°ê²° (YOLOX ì¬ì‚¬ìš©)
                if let rtmRunner = analyzer.rtmPoseRunner {
                    self?.personDetector.setRTMPoseRunner(rtmRunner)
                }
            }
        }
    }


    // MARK: - Helper Methods

    /// ì—¬ë°± ê³„ì‚° (RTMPose êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
    private func calculatePaddingFromKeypoints(
        keypoints: [(point: CGPoint, confidence: Float)]
    ) -> ImagePadding? {
        // êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš© (0-16: ëª¸í†µ í‚¤í¬ì¸íŠ¸, ì†ê°€ë½/ì–¼êµ´ ëœë“œë§ˆí¬ ì œì™¸)
        let structuralIndices = PhotographyFramingAnalyzer.StructuralKeypoints.all

        // ì‹ ë¢°ë„ 0.3 ì´ìƒì¸ í‚¤í¬ì¸íŠ¸ë§Œ í•„í„°ë§
        let validPoints = structuralIndices.compactMap { idx -> CGPoint? in
            guard idx < keypoints.count else { return nil }
            return keypoints[idx].confidence > 0.3 ? keypoints[idx].point : nil
        }

        // ìµœì†Œ 3ê°œ ì´ìƒì˜ í‚¤í¬ì¸íŠ¸ê°€ í•„ìš”
        guard validPoints.count >= 3 else { return nil }

        // ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (ì •ê·œí™”ëœ ì¢Œí‘œ: 0.0 ~ 1.0)
        let minX = validPoints.map { $0.x }.min() ?? 0
        let maxX = validPoints.map { $0.x }.max() ?? 1
        let minY = validPoints.map { $0.y }.min() ?? 0
        let maxY = validPoints.map { $0.y }.max() ?? 1

        // ì—¬ë°± ê³„ì‚° (ì •ê·œí™”ëœ ì¢Œí‘œê³„)
        let top = 1.0 - maxY     // ìƒë‹¨ ì—¬ë°±
        let bottom = minY        // í•˜ë‹¨ ì—¬ë°±
        let left = minX          // ì¢Œì¸¡ ì—¬ë°±
        let right = 1.0 - maxX   // ìš°ì¸¡ ì—¬ë°±

        return ImagePadding(
            top: top,
            bottom: bottom,
            left: left,
            right: right
        )
    }

    /// ğŸ—‘ï¸ êµ¬ì‹ ì—¬ë°± ê³„ì‚° (ì–¼êµ´ ìœ„ì¹˜ ê¸°ë°˜ bodyRect ì¶”ì •) - ë” ì´ìƒ ì‚¬ìš© ì•ˆí•¨
    @available(*, deprecated, message: "Use calculatePaddingFromKeypoints instead")
    private func calculatePadding(bodyRect: CGRect?, imageSize: CGSize) -> ImagePadding? {
        guard let body = bodyRect else { return nil }

        // ğŸ”¥ Vision ì¢Œí‘œê³„: Y=0(í™”ë©´ í•˜ë‹¨), Y=1(í™”ë©´ ìƒë‹¨)
        // body.minY = ì¸ë¬¼ì˜ ì•„ë˜ìª½ ê²½ê³„ (Y ì‘ì€ ê°’)
        // body.maxY = ì¸ë¬¼ì˜ ìœ„ìª½ ê²½ê³„ (Y í° ê°’)

        let top = 1.0 - body.maxY  // í™”ë©´ ìƒë‹¨ ì—¬ë°± (ì¸ë¬¼ ìœ„ ê³µê°„)
        let bottom = body.minY     // í™”ë©´ í•˜ë‹¨ ì—¬ë°± (ì¸ë¬¼ ì•„ë˜ ê³µê°„)
        let left = body.minX       // ì¢Œì¸¡ ì—¬ë°±
        let right = 1.0 - body.maxX  // ìš°ì¸¡ ì—¬ë°±

        return ImagePadding(
            top: top,
            bottom: bottom,
            left: left,
            right: right
        )
    }

    /// ğŸ†• v6: í‚¤í¬ì¸íŠ¸ì—ì„œ ì¸ë¬¼ ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (Python improved_margin_analyzer._calculate_person_bbox ì´ì‹)
    /// - Returns: ì •ê·œí™”ëœ ì¢Œí‘œ (0.0 ~ 1.0)ì˜ ë°”ìš´ë”© ë°•ìŠ¤
    private func calculateBodyRectFromKeypoints(_ keypoints: [(point: CGPoint, confidence: Float)], imageSize: CGSize) -> CGRect? {
        // ì‹ ë¢°ë„ 0.3 ì´ìƒì¸ êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸(0-16)ë§Œ í•„í„°ë§
        let structuralIndices = PhotographyFramingAnalyzer.StructuralKeypoints.all

        let validPoints = structuralIndices.compactMap { idx -> CGPoint? in
            guard idx < keypoints.count else { return nil }
            return keypoints[idx].confidence > 0.3 ? keypoints[idx].point : nil
        }

        // ìµœì†Œ 3ê°œ ì´ìƒì˜ í‚¤í¬ì¸íŠ¸ê°€ í•„ìš”
        guard validPoints.count >= 3 else { return nil }

        // ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚° (í”½ì…€ ì¢Œí‘œ)
        let minX = validPoints.map { $0.x }.min() ?? 0
        let maxX = validPoints.map { $0.x }.max() ?? 1
        let minY = validPoints.map { $0.y }.min() ?? 0
        let maxY = validPoints.map { $0.y }.max() ?? 1

        // ğŸ†• ì •ê·œí™” (0.0 ~ 1.0)
        let normalizedX = minX / imageSize.width
        let normalizedY = minY / imageSize.height
        let normalizedWidth = (maxX - minX) / imageSize.width
        let normalizedHeight = (maxY - minY) / imageSize.height

        return CGRect(x: normalizedX, y: normalizedY, width: normalizedWidth, height: normalizedHeight)
    }

    // MARK: - ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ë¶„ì„
    func analyzeReference(_ image: UIImage, imageData: Data? = nil) {
        print("ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘...")

        // ğŸ†• EXIF ì¶”ì¶œìš© ì´ë¯¸ì§€ ë°ì´í„° ì €ì¥
        self.referenceImageData = imageData ?? image.jpegData(compressionQuality: 1.0)

        guard let cgImage = image.cgImage else {
            print("âŒ cgImage ì—†ìŒ")
            return
        }

        // ğŸ†• ëª¨ë¸ ë¡œë”© ëŒ€ê¸°
        guard let analyzer = poseMLAnalyzer else {
            print("â³ PoseMLAnalyzer ë¡œë”© ì¤‘... ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ëŒ€ê¸°")
            // 0.5ì´ˆ í›„ ì¬ì‹œë„
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
                self?.analyzeReference(image)
            }
            return
        }

        // print("ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ í¬ê¸°: \(cgImage.width) x \(cgImage.height)")
        // print("ğŸ¯ ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ orientation: \(image.imageOrientation.rawValue)")

        // ğŸ”¥ RTMPoseë¡œ ì–¼êµ´+í¬ì¦ˆ ë™ì‹œ ë¶„ì„ (ONNX Runtime with CoreML EP)
        // print("ğŸ¯ PoseMLAnalyzer.analyzeFaceAndPose() í˜¸ì¶œ ì¤‘...")
        let (faceResult, poseResult) = analyzer.analyzeFaceAndPose(from: image)
        // print("ğŸ¯ ë¶„ì„ ì™„ë£Œ:")
        // print("   - ì–¼êµ´: \(faceResult != nil ? "âœ… ê²€ì¶œë¨" : "âŒ ê²€ì¶œ ì•ˆë¨")")
        // print("   - í¬ì¦ˆ: \(poseResult != nil ? "âœ… ê²€ì¶œë¨ (\(poseResult!.keypoints.count)ê°œ í‚¤í¬ì¸íŠ¸)" : "âŒ ê²€ì¶œ ì•ˆë¨")")

        if let pose = poseResult {
            let visibleCount = pose.keypoints.filter { $0.confidence >= 0.5 }.count
            // print("   - í¬ì¦ˆ ì‹ ë¢°ë„ â‰¥ 0.5: \(visibleCount)/\(pose.keypoints.count)ê°œ")
        }

        // ğŸ”¥ ë””ë²„ê·¸: í¬ì¦ˆ ê²€ì¶œ ì‹¤íŒ¨ ì‹œ ì´ë¯¸ì§€ ì €ì¥
        if poseResult == nil {
            saveDebugImage(image, reason: "pose_detection_failed")
        }

        let faceRect = faceResult?.faceRect
        let faceYaw = faceResult?.yaw
        let facePitch = faceResult?.pitch
        let poseKeypoints = poseResult?.keypoints

        // ë°ê¸° ê³„ì‚°
        let brightness = poseMLAnalyzer.calculateBrightness(from: cgImage)

        // ğŸ†• ë”ì¹˜ í‹¸íŠ¸ ê°ì§€ (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        let tiltAngle = cameraAngleDetector.detectDutchTilt(faceObservation: nil) ?? 0.0

        // ì „ì‹  ì˜ì—­ ì¶”ì •
        let bodyRect = poseMLAnalyzer.estimateBodyRect(from: faceRect)

        // ì¹´ë©”ë¼ ì•µê¸€ ê°ì§€ (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        let cameraAngle = cameraAngleDetector.detectCameraAngle(
            faceRect: faceRect,
            facePitch: facePitch,
            faceObservation: nil
        )

        // êµ¬ë„ íƒ€ì… ë¶„ë¥˜
        var compositionType: CompositionType? = nil
        if let faceRect = faceRect {
            let subjectPosition = CGPoint(x: faceRect.midX, y: faceRect.midY)
            compositionType = compositionAnalyzer.classifyComposition(subjectPosition: subjectPosition)
        }

        // ğŸ—‘ï¸ ì‹œì„  ì¶”ì  ë¹„í™œì„±í™” (VNFaceObservation ì œê±°)
        let gaze: GazeResult? = nil

        // ğŸ”¥ Depth Anything ML ê¸°ë°˜ ê¹Šì´ ì¶”ì • (ì™„ì „ ë¹„ë™ê¸° ì²˜ë¦¬)
        // âœ… ì„¸ë§ˆí¬ì–´ ì œê±°: ë°±ê·¸ë¼ìš´ë“œ íì—ì„œ ë¹„ë™ê¸° ì²´ì¸ìœ¼ë¡œ ì²˜ë¦¬
        // âš ï¸ ë©”ëª¨ë¦¬ ìµœì í™”: autoreleasepoolë¡œ ì„ì‹œ ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ

        // ğŸ†• ë¹„ìœ¨ ê°ì§€ (ë¨¼ì € ê³„ì‚°)
        let imageSize = CGSize(width: cgImage.width, height: cgImage.height)
        let aspectRatio = CameraAspectRatio.detect(from: imageSize)

        // ğŸ” ë””ë²„ê·¸: ë ˆí¼ëŸ°ìŠ¤ ì´ë¯¸ì§€ ë¹„ìœ¨ ê°ì§€ ê²°ê³¼
        let longSide = max(imageSize.width, imageSize.height)
        let shortSide = min(imageSize.width, imageSize.height)
        let rawRatio = longSide / shortSide
        print("ğŸ“ [ë ˆí¼ëŸ°ìŠ¤ ë¹„ìœ¨] ì´ë¯¸ì§€: \(Int(imageSize.width))x\(Int(imageSize.height)) â†’ ë¹„ìœ¨: \(String(format: "%.3f", rawRatio)) â†’ ê°ì§€: \(aspectRatio.displayName)")

        // ğŸ†• ì—¬ë°± ê³„ì‚° (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        // ğŸ”§ RTMPoseê°€ ì´ë¯¸ ì •ê·œí™”ëœ ì¢Œí‘œ(0.0~1.0)ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        var padding: ImagePadding? = nil
        if let keypoints = poseKeypoints, keypoints.count >= 17 {
            // êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸(0-16)ë¡œ ì—¬ë°± ê³„ì‚°
            padding = calculatePaddingFromKeypoints(keypoints: keypoints)
        }

        // ğŸ†• ì‚¬ì§„í•™ ê¸°ë°˜ í”„ë ˆì´ë° ë¶„ì„ (RTMPose 133ê°œ í‚¤í¬ì¸íŠ¸)
        // ğŸ”§ RTMPoseê°€ ì´ë¯¸ ì •ê·œí™”ëœ ì¢Œí‘œ(0.0~1.0)ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if let keypoints = poseKeypoints, keypoints.count >= 133 {
            referenceFramingResult = photographyFramingAnalyzer.analyze(
                keypoints: keypoints,
                imageSize: imageSize
            )
            if let refFraming = referenceFramingResult {
                // print("   - ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ ìƒ· íƒ€ì…: \(refFraming.shotType.rawValue)")
                // print("   - ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ í—¤ë“œë£¸: \(String(format: "%.1f%%", refFraming.headroom * 100))")
                // print("   - ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ ì¹´ë©”ë¼ ì•µê¸€: \(refFraming.cameraAngle.rawValue)")
            }
        } else {
            referenceFramingResult = nil
            // print("   - âš ï¸ ì‚¬ì§„í•™ í”„ë ˆì´ë° ë¶„ì„ ë¶ˆê°€ (í‚¤í¬ì¸íŠ¸ ë¶€ì¡±)")
        }

        // ğŸ”¥ ë¹„ë™ê¸° ì²´ì¸ ì‹œì‘: Depth ì¶”ì • â†’ PersonDetector â†’ ìµœì¢… ë¶„ì„ ì™„ë£Œ
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else { return }

            autoreleasepool {
                // Step 1: Depth ì¶”ì • (ë¹„ë™ê¸°)
                self.depthAnything.estimateDepth(from: image) { [weak self] result in
                    guard let self = self else { return }

                    let depth: V15DepthResult?
                    switch result {
                    case .success(let depthResult):
                        depth = depthResult
                        // print("âœ… Depth Anything ë¶„ì„ ì™„ë£Œ: ì••ì¶•ê° \(String(format: "%.2f", depthResult.compressionIndex))")
                    case .failure(let error):
                        // print("âš ï¸ Depth Anything ë¶„ì„ ì‹¤íŒ¨: \(error.localizedDescription)")
                        depth = nil
                    }

                    // Step 2: PersonDetector (ë¹„ë™ê¸°)
                    if let ciImage = CIImage(image: image) {
                        self.personDetector.detectPerson(in: ciImage) { [weak self] preciseBBox in
                            guard let self = self else { return }

                            // Step 3: ìµœì¢… ë¶„ì„ ì™„ë£Œ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
                            self.finalizeReferenceAnalysis(
                                faceRect: faceRect,
                                bodyRect: bodyRect,
                                brightness: Double(brightness),
                                tiltAngle: Double(tiltAngle),
                                faceYaw: faceYaw.map { Double($0) },
                                facePitch: facePitch.map { Double($0) },
                                cameraAngle: cameraAngle,
                                poseKeypoints: poseKeypoints,
                                compositionType: compositionType,
                                gaze: gaze,
                                depth: depth,
                                aspectRatio: aspectRatio,
                                padding: padding,
                                preciseBBox: preciseBBox,
                                image: image,
                                imageSize: imageSize
                            )
                        }
                    } else {
                        // PersonDetector ì‹¤í–‰ ë¶ˆê°€ ì‹œ ë°”ë¡œ ì™„ë£Œ
                        self.finalizeReferenceAnalysis(
                            faceRect: faceRect,
                            bodyRect: bodyRect,
                            brightness: Double(brightness),
                            tiltAngle: Double(tiltAngle),
                            faceYaw: faceYaw.map { Double($0) },
                            facePitch: facePitch.map { Double($0) },
                            cameraAngle: cameraAngle,
                            poseKeypoints: poseKeypoints,
                            compositionType: compositionType,
                            gaze: gaze,
                            depth: depth,
                            aspectRatio: aspectRatio,
                            padding: padding,
                            preciseBBox: nil,
                            image: image,
                            imageSize: imageSize
                        )
                    }
                }
            }
        }
    }

    // MARK: - ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ìµœì¢… ì²˜ë¦¬ (ë¹„ë™ê¸° ì™„ë£Œ í›„)
    private func finalizeReferenceAnalysis(
        faceRect: CGRect?,
        bodyRect: CGRect?,
        brightness: Double,
        tiltAngle: Double,
        faceYaw: Double?,
        facePitch: Double?,
        cameraAngle: CameraAngle,
        poseKeypoints: [(point: CGPoint, confidence: Float)]?,
        compositionType: CompositionType?,
        gaze: GazeResult?,
        depth: V15DepthResult?,
        aspectRatio: CameraAspectRatio,
        padding: ImagePadding?,
        preciseBBox: CGRect?,
        image: UIImage,
        imageSize: CGSize
    ) {
        // ë°±ê·¸ë¼ìš´ë“œ íì—ì„œ ì‹¤í–‰ë¨

        // ğŸ†• v1.5: ì—¬ë°± ë¶„ì„ ë° ìºì‹±
        // ğŸ”§ bboxê°€ ì—†ì–´ë„ cachedReferenceëŠ” í•­ìƒ ì„¤ì • (ë¹„ìœ¨ ê²Œì´íŠ¸ ë“± ë™ì‘ ë³´ì¥)
        let bbox = preciseBBox ?? bodyRect ?? CGRect(x: 0.1, y: 0.1, width: 0.8, height: 0.8)  // ê¸°ë³¸ê°’: ì´ë¯¸ì§€ ì¤‘ì•™ 80%

        let marginResult = marginAnalyzer.analyze(
            bbox: bbox,
            imageSize: imageSize,
            isNormalized: true
        )

        // ìºì‹œ ì €ì¥
        let refId = UUID().uuidString
        let cachedRef = CacheManager.shared.cacheReference(
            id: refId,
            image: image,
            bbox: bbox,
            margins: marginResult,
            compressionIndex: depth.map { CGFloat($0.compressionIndex) }
        )

        // ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ìºì‹œ ì—…ë°ì´íŠ¸
        DispatchQueue.main.async { [weak self] in
            self?.cachedReference = cachedRef
        }

        if preciseBBox == nil && bodyRect == nil {
            // print("âš ï¸ ë ˆí¼ëŸ°ìŠ¤ BBox ì—†ìŒ â†’ ê¸°ë³¸ê°’ ì‚¬ìš© (ë¹„ìœ¨ ê²Œì´íŠ¸ëŠ” ë™ì‘)")
        }

        // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬ ì¶”ì • (EXIF â†’ ëìŠ¤ë§µ ìˆœì„œ)
        let refFL = focalLengthEstimator.estimateReferenceFocalLength(
            imageData: referenceImageData,
            depthMap: referenceDepthMap,
            fallback: 50
        )

        // ğŸ“¸ ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ìš”ì•½ (í•œ ì¤„)
        // ğŸ”§ ìƒ·íƒ€ì…ì€ ShotTypeGate.fromKeypoints() ê¸°ì¤€ìœ¼ë¡œ í†µì¼ (GateSystem/SimpleGuideì™€ ë™ì¼)
        let shotTypeStr: String
        if let keypoints = poseKeypoints {
            let poseKeypointsConverted = keypoints.map { PoseKeypoint(location: $0.point, confidence: $0.confidence) }
            shotTypeStr = ShotTypeGate.fromKeypoints(poseKeypointsConverted).displayName
        } else {
            shotTypeStr = "ë¶„ì„ì‹¤íŒ¨"
        }
        let compressionStr = depth.map { String(format: "%.2f", $0.compressionIndex) } ?? "N/A"
        let keypointCount = poseKeypoints?.filter { $0.confidence >= 0.5 }.count ?? 0

        print("ğŸ“¸ [ë ˆí¼ëŸ°ìŠ¤] ë¹„ìœ¨:\(aspectRatio.displayName) | ìƒ·íƒ€ì…:\(shotTypeStr) | ì••ì¶•:\(compressionStr) | ì´ˆì :\(refFL.focalLength35mm)mm | í‚¤í¬ì¸íŠ¸:\(keypointCount)ê°œ")

        // ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ referenceAnalysis ë° referenceFocalLength ì—…ë°ì´íŠ¸
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }

            self.referenceAnalysis = FrameAnalysis(
                faceRect: faceRect,
                bodyRect: bodyRect,
                brightness: Float(brightness),
                tiltAngle: Float(tiltAngle),
                faceYaw: faceYaw.map { Float($0) },
                facePitch: facePitch.map { Float($0) },
                cameraAngle: cameraAngle,
                poseKeypoints: poseKeypoints,
                compositionType: compositionType,
                gaze: gaze,
                depth: depth,
                aspectRatio: aspectRatio,
                imagePadding: padding
            )

            self.referenceFocalLength = refFL

            // ğŸ†• ëª©í‘œ ì¤Œ ë°°ìœ¨ ê³„ì‚° ë° ê³ ì • (í•œ ë²ˆë§Œ!)
            // ë ˆí¼ëŸ°ìŠ¤ê°€ 50mmë¡œ ì°í˜”ê³ , iPhone ê¸°ë³¸ì´ 24mmë¼ë©´ â†’ 50/24 â‰ˆ 2.1x ì¤Œ í•„ìš”
            if refFL.focalLength35mm > FocalLengthEstimator.iPhoneBaseFocalLength {
                let targetZoom = CGFloat(refFL.focalLength35mm) / CGFloat(FocalLengthEstimator.iPhoneBaseFocalLength)
                self.targetZoomFactor = targetZoom
                print("ğŸ“ [ëª©í‘œ ì¤Œ ì„¤ì •] \(String(format: "%.1fx", targetZoom)) (ë ˆí¼ëŸ°ìŠ¤ \(refFL.focalLength35mm)mm)")
            } else {
                // ë ˆí¼ëŸ°ìŠ¤ê°€ ê´‘ê°ì´ë©´ 1xë¡œ ê³ ì •
                self.targetZoomFactor = 1.0
                print("ğŸ“ [ëª©í‘œ ì¤Œ ì„¤ì •] 1.0x (ë ˆí¼ëŸ°ìŠ¤ ê´‘ê° \(refFL.focalLength35mm)mm)")
            }

            // ğŸ†• SimpleRealTimeGuideì— ë ˆí¼ëŸ°ìŠ¤ ì„¤ì • (ì¤Œ ì •ë³´ í¬í•¨)
            if let keypoints = poseKeypoints {
                let poseKeypointsConverted = keypoints.map { PoseKeypoint(location: $0.point, confidence: $0.confidence) }
                self.simpleRealTimeGuide.setReference(
                    keypoints: poseKeypointsConverted,
                    imageSize: imageSize,
                    zoomFactor: self.targetZoomFactor  // ğŸ†• ëª©í‘œ ì¤Œ ì „ë‹¬
                )
            }

            print("âœ… ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ì™„ë£Œ - ì‹¤ì‹œê°„ í”¼ë“œë°± ëª¨ë“œ ì¤€ë¹„!")
        }
    }

    // MARK: - Pause/Resume (íƒ­ ì „í™˜ìš©)
    func pauseAnalysis() {
        // print("â¸ï¸ RealtimeAnalyzer: ë¶„ì„ ì¼ì‹œ ì¤‘ì§€ (íƒ­ ì „í™˜)")
        isPaused = true

        // í”¼ë“œë°± ì´ˆê¸°í™”
        DispatchQueue.main.async {
            var newState = self.state
            newState.instantFeedback = []
            newState.isPerfect = false
            newState.perfectScore = 0.0
            newState.unifiedFeedback = nil
            newState.activeFeedback = nil  // ğŸ†• í™œì„± í”¼ë“œë°± ì´ˆê¸°í™”
            self.state = newState
        }
    }

    func resumeAnalysis() {
        // print("â–¶ï¸ RealtimeAnalyzer: ë¶„ì„ ì¬ê°œ (íƒ­ ë³µê·€)")
        isPaused = false

        // ìƒíƒœ ì´ˆê¸°í™” (ìƒˆë¡­ê²Œ ì‹œì‘)
        lastAnalysisTime = Date()
        feedbackHistory.removeAll()
        disappearedFeedbackHistory.removeAll()
        perfectFrameCount = 0
    }

    /// ğŸ†• ì´¬ì˜ ì™„ë£Œ í›„ Temporal Lock ë¦¬ì…‹ (ì—°ì† ì´¬ì˜ ë°©ì§€)
    func resetAfterCapture() {
        print("ğŸ“· ì´¬ì˜ ì™„ë£Œ - Temporal Lock ë¦¬ì…‹")
        stabilityState = .idle

        DispatchQueue.main.async {
            var newState = self.state
            newState.stabilityProgress = 0.0
            newState.isPerfect = false
            self.state = newState
        }
    }

    // MARK: - ì‹¤ì‹œê°„ í”„ë ˆì„ ë¶„ì„

    
    // MARK: - Internal Analysis Logic
    // Renamed from analyzeFrame to separate public/private concerns if needed.
    // Kept public analyzeFrame for legacy calls if any, but logic moved here.
    func analyzeFrame(_ image: UIImage, isFrontCamera: Bool = false, currentAspectRatio: CameraAspectRatio = .ratio4_3) {
        // Adapter for old timer-based calls (will be removed, but kept for safety during refactor)
        guard !isAnalyzing, !isPaused else { return }
        guard Date().timeIntervalSince(lastAnalysisTime) >= thermalManager.recommendedAnalysisInterval else { return }
        isAnalyzing = true
        lastAnalysisTime = Date()
        
        analysisQueue.async { [weak self] in
            self?.analyzeFrameInternal(image, isFrontCamera: isFrontCamera, currentAspectRatio: currentAspectRatio, brightness: nil)
        }
    }

    private func analyzeFrameInternal(_ image: UIImage, isFrontCamera: Bool, currentAspectRatio: CameraAspectRatio, brightness: Double?) {
        // ğŸ†• Environment Check (Gate 0.5)
        if let b = brightness, b < -2.0 {
            DispatchQueue.main.async {
                var newState = self.state
                newState.environmentWarning = "ë„ˆë¬´ ì–´ë‘ì›Œìš” ğŸ’¡"
                newState.isPerfect = false
                newState.stabilityProgress = 0.0
                // Gate í‰ê°€ ì¤‘ë‹¨ì€ ì•„ë‹ˆì§€ë§Œ ê²½ê³  í‘œì‹œ
                self.state = newState
            }
            // ë„ˆë¬´ ì–´ë‘ìš°ë©´ ë¶„ì„ ì¤‘ë‹¨? (ì‚¬ìš©ì ê²½í—˜ìƒ ê³„ì† ë¶„ì„í•˜ëŠ”ê²Œ ë‚˜ì„ ìˆ˜ë„ ìˆì§€ë§Œ, ì •í™•ë„ê°€ ë–¨ì–´ì§)
            // ì—¬ê¸°ì„œëŠ” ê²½ê³ ë§Œ ë„ìš°ê³  ë¶„ì„ì€ ì§„í–‰ (ë‹¨, ê²°ê³¼ ì‹ ë¢°ë„ê°€ ë‚®ìŒ)
        } else {
             // ê²½ê³  í•´ì œëŠ” processAnalysisResultì—ì„œ ì²˜ë¦¬ ë˜ëŠ” state ì—…ë°ì´íŠ¸ ì‹œ
             // í•˜ì§€ë§Œ ì—¬ê¸°ì„œ asyncë¡œ í•´ì œí•˜ë©´ íƒ€ì´ë° ì´ìŠˆê°€ ìˆì„ ìˆ˜ ìˆìŒ.
             // processAnalysisResultê¹Œì§€ ì „ë‹¬í•´ì„œ ì²˜ë¦¬í•˜ëŠ”ê²Œ ì•ˆì „.
        }
        // Safe check for reference
        guard let reference = referenceAnalysis else {
            // ğŸ”§ DEBUG: referenceAnalysis nil ì›ì¸ ì¶”ì 
            // print("â­ï¸ ì‹¤ì‹œê°„ ë¶„ì„ ìŠ¤í‚µ: referenceAnalysis nil (ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„ ëŒ€ê¸° ì¤‘)")
            DispatchQueue.main.async {
                var newState = self.state
                newState.instantFeedback = []
                newState.perfectScore = 0.0
                newState.isPerfect = false
                self.state = newState
                self.isAnalyzing = false
            }
            return
        }
        
         guard let cgImage = image.cgImage else {
             resetAnalyzingFlag()
             return 
         }

        // ğŸ†• ëª¨ë¸ ë¡œë”© ëŒ€ê¸° (ì•± ì‹œì‘ ì§í›„)
        guard let analyzer = self.poseMLAnalyzer else {
            // print("â³ PoseMLAnalyzer ë¡œë”© ì¤‘... ë¶„ì„ ìŠ¤í‚µ")
            resetAnalyzingFlag()
            return
        }

        let analysisStart = CACurrentMediaTime()  // ğŸ” í”„ë¡œíŒŒì¼ë§

        // ğŸ†• YOLOX + RTMPose ë¶„ë¦¬ ì‹¤í–‰
        // - YOLOX: ë§¤ í”„ë ˆì„ (~30ms) â†’ ì¸ë¬¼ BBox
        // - RTMPose: 3í”„ë ˆì„ë§ˆë‹¤ (~175ms) â†’ í‚¤í¬ì¸íŠ¸ (ìºì‹œ ì‚¬ìš©)

        var faceResult: FaceAnalysisResult? = nil
        var poseResult: PoseAnalysisResult? = nil

        // ğŸ”¥ RTMPose ì§ì ‘ ì‹¤í–‰ (YOLOX ì˜ì¡´ ì œê±°)
        // RTMPoseê°€ YOLOX ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ì „ì²´ ì´ë¯¸ì§€ì—ì„œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ
        // ìƒë°˜ì‹ , ë¬´ë¦ìƒ· ë“± ë¶€ë¶„ ì¸ë¬¼ë„ ê²€ì¶œ ê°€ëŠ¥

        let shouldRunRTMPose = (frameCount % rtmPoseInterval == 0) || lastPoseResult == nil

        if shouldRunRTMPose {
            let poseStart = CACurrentMediaTime()
            let (face, pose) = analyzer.analyzeFaceAndPose(from: image)
            faceResult = face
            poseResult = pose

            // ìºì‹œ ì—…ë°ì´íŠ¸
            if let pose = pose {
                self.lastPoseResult = pose
                self.lastPoseKeypoints = pose.keypoints

                // ğŸ†• RTMPose í‚¤í¬ì¸íŠ¸ì—ì„œ BBox ê³„ì‚° (YOLOX ëŒ€ì²´)
                if let bbox = ShotTypeGate.calculateKeypointBBox(
                    pose.keypoints.map { PoseKeypoint(location: $0.point, confidence: $0.confidence) }
                ) {
                    self.lastYOLOXBBox = bbox
                }
            }

            let poseTime = (CACurrentMediaTime() - poseStart) * 1000
            // print("ğŸ“Š [RTMPose] \(String(format: "%.1f", poseTime))ms (í”„ë ˆì„ \(frameCount))")
        } else {
            // ìºì‹œëœ í‚¤í¬ì¸íŠ¸ ì‚¬ìš©
            poseResult = lastPoseResult
            // print("ğŸ“¦ [RTMPose ìºì‹œ] í”„ë ˆì„ \(frameCount)")
        }

        let analysisEnd = CACurrentMediaTime()  // ğŸ”

        // ğŸ” í”„ë¡œíŒŒì¼ë§ ë¡œê·¸
        let totalTime = (analysisEnd - analysisStart) * 1000
        // print("ğŸ“Š [RealtimeAnalyzer] ì´ë¶„ì„: \(String(format: "%.1f", totalTime))ms")

        // ë¶„ì„ ì™„ë£Œ í›„ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ UI ì—…ë°ì´íŠ¸
        DispatchQueue.main.async {
            self.isAnalyzing = false
            self.processAnalysisResult(
                faceResult: faceResult,
                poseResult: poseResult,
                cgImage: cgImage,
                reference: reference, // Passed safely
                isFrontCamera: isFrontCamera,
                currentAspectRatio: currentAspectRatio
            )
        }
    }

    // MARK: - ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ (ë©”ì¸ ìŠ¤ë ˆë“œ)
    private func processAnalysisResult(
        faceResult: FaceAnalysisResult?,
        poseResult: PoseAnalysisResult?,
        cgImage: CGImage,
        reference: FrameAnalysis,
        isFrontCamera: Bool,
        currentAspectRatio: CameraAspectRatio
    ) {
        // ğŸ†• v1.5: í”„ë ˆì„ ì¹´ìš´í„° ì¦ê°€
        frameCount += 1

        // ğŸ”¥ ì„±ëŠ¥ ë¡œê·¸ (10ì´ˆë§ˆë‹¤)
        if Date().timeIntervalSince(lastPerformanceLog) >= 10 {
            lastPerformanceLog = Date()
            // print(PerformanceOptimizer.shared.getPerformanceReport())
            // print("ğŸŒ¡ï¸ ë°œì—´ ìƒíƒœ: \(thermalManager.currentThermalState.rawValue), ë¶„ì„ ê°„ê²©: \(Int(thermalManager.recommendedAnalysisInterval * 1000))ms")
        }

        // ğŸ†• ì¢…íš¡ë¹„ ì²´í¬ëŠ” ì–¼êµ´ ê°ì§€ì™€ ë¬´ê´€í•˜ê²Œ í•­ìƒ ìˆ˜í–‰
        // Gate 0 (ì¢…íš¡ë¹„)ëŠ” ê°€ì¥ ë¨¼ì € ì²´í¬ë˜ì–´ì•¼ í•¨
        let aspectRatioMatched = (currentAspectRatio == reference.aspectRatio)

        // ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì™„ì„±ë„ 0ìœ¼ë¡œ ì„¤ì •
        guard faceResult != nil else {
            // Update grouped state
            var newState = self.state

            // ğŸ”¥ ì¢…íš¡ë¹„ ë¶ˆì¼ì¹˜ ì‹œ: ì¢…íš¡ë¹„ í”¼ë“œë°±ë§Œ í‘œì‹œ
            if !aspectRatioMatched {
                // Gate 0ë§Œ í¬í•¨ëœ ìµœì†Œ GateEvaluation ìƒì„±
                let gate0Result = GateResult(
                    name: "ë¹„ìœ¨",
                    score: 0.0,
                    threshold: 1.0,
                    feedback: "ì¹´ë©”ë¼ ë¹„ìœ¨ì„ \(reference.aspectRatio.displayName)ë¡œ ë³€ê²½í•˜ì„¸ìš”",
                    icon: "ğŸ“",
                    category: "aspect_ratio",
                    debugInfo: "í˜„ì¬: \(currentAspectRatio.displayName) vs ëª©í‘œ: \(reference.aspectRatio.displayName)"
                )
                let dummyGate = GateResult(name: "-", score: 0, threshold: 1, feedback: "", icon: "", category: "")
                newState.gateEvaluation = GateEvaluation(
                    gate0: gate0Result,
                    gate1: dummyGate,
                    gate2: dummyGate,
                    gate3: dummyGate,
                    gate4: dummyGate
                )
                newState.instantFeedback = []
                print("ğŸ“ [No Face] ì¢…íš¡ë¹„ ë¶ˆì¼ì¹˜: \(currentAspectRatio.displayName) vs \(reference.aspectRatio.displayName)")
            } else {
                // ì¢…íš¡ë¹„ëŠ” ë§ì§€ë§Œ ì–¼êµ´ ì—†ìŒ
                newState.instantFeedback = [FeedbackItem(
                    priority: 1,
                    icon: "ğŸ‘¤",
                    message: "ì–¼êµ´ì„ í™”ë©´ì— ë³´ì—¬ì£¼ì„¸ìš”",
                    category: "no_face",
                    currentValue: nil,
                    targetValue: nil,
                    tolerance: nil,
                    unit: nil
                )]
                newState.gateEvaluation = nil
            }

            newState.perfectScore = 0.0
            newState.isPerfect = false

            if self.state != newState {
                self.state = newState
            }
            return
        }

        // ë°ê¸° ë° ê¸°ìš¸ê¸°
        let brightness = poseMLAnalyzer.calculateBrightness(from: cgImage)
        let tilt = cameraAngleDetector.detectDutchTilt(faceObservation: nil) ?? 0.0

        // ğŸ†• ì´ë¯¸ì§€ í¬ê¸° (ì •ê·œí™”ì— í•„ìš”)
        let imageSize = CGSize(width: cgImage.width, height: cgImage.height)

        // ğŸ†• ì „ì‹  ì˜ì—­ - RTMPose í‚¤í¬ì¸íŠ¸ì—ì„œ ì •í™•í•˜ê²Œ ê³„ì‚° (ì •ê·œí™”ëœ ì¢Œí‘œ)
        let bodyRect: CGRect? = {
            if let keypoints = poseResult?.keypoints, !keypoints.isEmpty {
                return calculateBodyRectFromKeypoints(keypoints, imageSize: imageSize)
            }
            // RTMPose í‚¤í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì–¼êµ´ ê¸°ë°˜ ì¶”ì • (fallback) - ì´ë¯¸ ì •ê·œí™”ë¨
            return poseMLAnalyzer.estimateBodyRect(from: faceResult?.faceRect)
        }()

        // ì¹´ë©”ë¼ ì•µê¸€ (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        let cameraAngle = cameraAngleDetector.detectCameraAngle(
            faceRect: faceResult?.faceRect,
            facePitch: faceResult?.pitch,
            faceObservation: nil
        )

        // êµ¬ë„
        var compositionType: CompositionType? = nil
        if let faceRect = faceResult?.faceRect {
            let subjectPosition = CGPoint(x: faceRect.midX, y: faceRect.midY)
            compositionType = compositionAnalyzer.classifyComposition(subjectPosition: subjectPosition)
        }

        // ğŸ—‘ï¸ ì‹œì„  ë¹„í™œì„±í™” (VNFaceObservation ì œê±°)
        let gaze: GazeResult? = nil

        // ğŸ”¥ Level 2: Depth Anything ML ê¹Šì´ ì¶”ì • (ë™ì  í”„ë ˆì„ ìŠ¤í‚µ)
        let depth: V15DepthResult? = lastDepthResult  // ìºì‹œëœ ê°’ ì‚¬ìš©
        if frameSkipper.shouldExecute(level: 2, frameCount: frameCount) {
            // ë™ì  ê°„ê²©ìœ¼ë¡œ ìƒˆë¡œ ê³„ì‚° (ë¹„ë™ê¸° â†’ ë°±ê·¸ë¼ìš´ë“œ)
            let uiImage = UIImage(cgImage: cgImage)
            depthAnything.estimateDepth(from: uiImage) { [weak self] result in
                DispatchQueue.main.async {
                    switch result {
                    case .success(let depthResult):
                        self?.lastDepthResult = depthResult  // ìºì‹œ ì—…ë°ì´íŠ¸
                    case .failure:
                        break  // ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ìºì‹œ ìœ ì§€
                    }
                }
            }
        }

        // ğŸ†• í˜„ì¬ ì´ë¯¸ì§€ í¬ê¸° (ìœ„ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨)
        let currentImageSize = imageSize

        // ğŸ†• ì—¬ë°± ê³„ì‚° (RTMPose í‚¤í¬ì¸íŠ¸ ê¸°ë°˜)
        var currentPadding: ImagePadding? = nil
        if let keypoints = poseResult?.keypoints, keypoints.count >= 17 {
            // í‚¤í¬ì¸íŠ¸ë¥¼ ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜ (0.0 ~ 1.0)
            let normalizedKeypoints = keypoints.map { kp -> (point: CGPoint, confidence: Float) in
                let normalizedPoint = CGPoint(
                    x: kp.point.x / currentImageSize.width,
                    y: kp.point.y / currentImageSize.height
                )
                return (point: normalizedPoint, confidence: kp.confidence)
            }
            // êµ¬ì¡°ì  í‚¤í¬ì¸íŠ¸(0-16)ë¡œ ì—¬ë°± ê³„ì‚°
            currentPadding = calculatePaddingFromKeypoints(keypoints: normalizedKeypoints)
        }

        // ğŸ†• í”„ë ˆì´ë° ë¶„ì„ ì¶”ê°€ (ìµœìš°ì„ )
        let _ = FrameAnalysis(
            faceRect: faceResult?.faceRect,
            bodyRect: bodyRect,
            brightness: brightness,
            tiltAngle: tilt,
            faceYaw: faceResult?.yaw,
            facePitch: faceResult?.pitch,
            cameraAngle: cameraAngle,
            poseKeypoints: poseResult?.keypoints,
            compositionType: compositionType,
            gaze: gaze,
            depth: depth,
            aspectRatio: currentAspectRatio,
            imagePadding: currentPadding
        )

        // ============================================
        // ğŸ†• v1.5 í†µí•© Gate System í‰ê°€ (5ë‹¨ê³„)
        // ============================================

        // ğŸ†• Level 3 YOLOX ì¤‘ë³µ í˜¸ì¶œ ì œê±°
        // YOLOXëŠ” ì´ë¯¸ analyzeFrameInternalì—ì„œ ë§¤ í”„ë ˆì„ ì‹¤í–‰ë¨ (lastYOLOXBBoxì— ì €ì¥)
        // ì—¬ê¸°ì„œ ë³„ë„ë¡œ í˜¸ì¶œí•  í•„ìš” ì—†ìŒ

        // ğŸ†• í˜„ì¬ BBox ê²°ì • - YOLOX ê²°ê³¼ ìš°ì„  ì‚¬ìš©
        // YOLOXëŠ” ë§¤ í”„ë ˆì„ ì‹¤í–‰ë˜ë¯€ë¡œ ê°€ì¥ ìµœì‹  BBoxì„
        let currentBBox: CGRect
        if let yoloxBBox = lastYOLOXBBox {
            // YOLOXì—ì„œ ì¸ë¬¼ ê°ì§€ë¨ â†’ ê°€ì¥ ì •í™•í•œ BBox
            currentBBox = yoloxBBox
        } else if let body = bodyRect {
            // YOLOX ì‹¤íŒ¨ ì‹œ Vision bodyRect ì‚¬ìš© (fallback)
            currentBBox = body
        } else {
            // ë‘˜ ë‹¤ ì¸ë¬¼ ì—†ìŒ â†’ ì‘ì€ ê¸°ë³¸ê°’ (ì¸ë¬¼ ë¯¸ê²€ì¶œë¡œ ì²˜ë¦¬ë¨)
            currentBBox = CGRect(x: 0.45, y: 0.45, width: 0.01, height: 0.01)
        }

        // ğŸ”§ FIX: ì••ì¶•ê°ì€ í˜„ì¬ í”„ë ˆì„ ê°’ ì‚¬ìš© (ìºì‹œ ì˜ì¡´ ì œê±°)
        // depthê°€ nilì´ë©´ ì••ì¶•ê°ë„ nilë¡œ ì „ë‹¬ â†’ Gateì—ì„œ "ë¶„ì„ ì¤‘" í‘œì‹œ
        let currentCompressionIndex: CGFloat?
        if let depthResult = depth {
            currentCompressionIndex = CGFloat(depthResult.compressionIndex)
            lastCompressionIndex = currentCompressionIndex  // ìºì‹œë„ ì—…ë°ì´íŠ¸
        } else {
            // ğŸ”§ ìºì‹œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - í˜„ì¬ í”„ë ˆì„ì— depth ì—†ìœ¼ë©´ nil
            currentCompressionIndex = nil
        }



        // âœ… ë¬´ê±°ìš´ ì—°ì‚°ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì´ë™
        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ Gate System í‰ê°€ ë° í”¼ë“œë°± ìƒì„±
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else { return }

            // ğŸ†• v6: í‚¤í¬ì¸íŠ¸ ë³€í™˜ (tuple â†’ PoseKeypoint)
            let currentPoseKeypoints: [PoseKeypoint]? = poseResult?.keypoints.map { kp in
                PoseKeypoint(location: kp.point, confidence: kp.confidence)
            }
            let referencePoseKeypoints: [PoseKeypoint]? = reference.poseKeypoints?.map { kp in
                PoseKeypoint(location: kp.point, confidence: kp.confidence)
            }

            // ğŸš€ Optimization: Move Pose Comparison to Background
            var poseComparison: PoseComparisonResult? = nil
            if let refKeypoints = reference.poseKeypoints,
               let curKeypoints = poseResult?.keypoints,
               refKeypoints.count >= 133 && curKeypoints.count >= 133 {
                poseComparison = self.poseComparator.comparePoses(
                    referenceKeypoints: refKeypoints,
                    currentKeypoints: curKeypoints
                )
            }

            var stableFeedback: [FeedbackItem] = []
            var evaluation: GateEvaluation?
            var unifiedFeedback: UnifiedFeedback?

            // ğŸ†• SimpleRealTimeGuide í‰ê°€ (GateSystem ëŒ€ì²´)
            let hasPersonDetected = currentBBox.height > 0.05  // ìµœì†Œ 5% ì´ìƒì´ë©´ ì¸ë¬¼ ê°ì§€
            let simpleGuideResult = self.simpleRealTimeGuide.evaluate(
                currentKeypoints: currentPoseKeypoints ?? [],
                hasPersonDetected: hasPersonDetected,
                isFrontCamera: isFrontCamera,
                currentZoom: self.currentZoomFactor  // ğŸ†• ì¤Œ ì •ë³´ ì „ë‹¬
            )

            if let cached = self.cachedReference {
                // ğŸ”§ DEBUG: Gate í‰ê°€ ì‹œì‘
                print("ğŸš¦ Gate ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘ (cachedReference ì¡´ì¬)")

                // ğŸ†• 35mm í™˜ì‚° ì´ˆì ê±°ë¦¬ ê³„ì‚°
                let currentFocalLength = self.focalLengthEstimator.focalLengthFromZoom(self.currentZoomFactor)

                // ğŸ†• Adaptive Difficulty ì ìš©
                self.gateSystem.difficultyMultiplier = self.frustrationMultiplier

                // ğŸ†• ëª©í‘œ ì¤Œê³¼ í˜„ì¬ ì¤Œì„ GateSystemì— ì „ë‹¬
                self.gateSystem.targetZoomFactor = self.targetZoomFactor
                self.gateSystem.currentZoomFactor = self.currentZoomFactor

                // ğŸ”¥ ë¬´ê±°ìš´ ì—°ì‚°: Gate System í‰ê°€ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
                evaluation = self.gateSystem.evaluate(
                    currentBBox: currentBBox,
                    referenceBBox: cached.bbox,
                    currentImageSize: currentImageSize,
                    referenceImageSize: cached.imageSize,
                    compressionIndex: currentCompressionIndex,
                    referenceCompressionIndex: cached.compressionIndex,
                    currentAspectRatio: currentAspectRatio,
                    referenceAspectRatio: reference.aspectRatio,
                    poseComparison: poseComparison,
                    isFrontCamera: isFrontCamera,
                    currentKeypoints: currentPoseKeypoints,
                    referenceKeypoints: referencePoseKeypoints,
                    currentFocalLength: currentFocalLength,
                    referenceFocalLength: self.referenceFocalLength
                )

                // ğŸ”¥ ë¬´ê±°ìš´ ì—°ì‚°: UnifiedFeedback ìƒì„± (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
                if let eval = evaluation {
                    let targetZoomValue = self.referenceFocalLength.map {
                        CGFloat($0.focalLength35mm) / CGFloat(FocalLengthEstimator.iPhoneBaseFocalLength)
                    }

                    unifiedFeedback = UnifiedFeedbackGenerator.shared.generateUnifiedFeedback(
                        from: eval,
                        isFrontCamera: isFrontCamera,
                        currentZoom: self.currentZoomFactor,
                        targetZoom: targetZoomValue,
                        targetSubjectSize: cached.bbox.width * cached.bbox.height
                    )
                    
                    // ğŸ†• UI í‘œì‹œìš© Debug String (Gate 1 - Shot Type)
                    // ğŸ†• UI í‘œì‹œìš© Debug String (Gate 1 - Shot Type)
                    // (RealtimeAnalyzer.process ë‚´ì—ì„œ ì§ì ‘ í• ë‹¹)

                    // ğŸ” DEBUG: Unified Feedback Generation
                    /*
                    if let unified = unifiedFeedback {
                         print("âœ¨ Unified Feedback Generated: [\(unified.primaryAction.rawValue)] \(unified.mainMessage)")
                    }
                    */

                    // Gate System í”¼ë“œë°± ìƒì„±
                    let gateFeedbacks = V15FeedbackGenerator.shared.generateFeedbackItems(from: eval)

                    // íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì ìš©
                    for fb in gateFeedbacks {
                        self.feedbackHistory[fb.category, default: 0] += 1
                    }

                    // íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ë° ì¢Œì ˆ ê°ì§€ (Adaptive Difficulty)
                    if eval.allPassed {
                        // ì„±ê³µ ì‹œ ë‚œì´ë„ ë° íƒ€ì´ë¨¸ ë¦¬ì…‹
                        self.frustrationMultiplier = 1.0
                        self.feedbackStartTimes.removeAll()
                    } else {
                        // í˜„ì¬ ì£¼ìš” í”¼ë“œë°± ì¶”ì 
                        let primary = eval.primaryFeedback
                        if self.feedbackStartTimes[primary] == nil {
                            self.feedbackStartTimes[primary] = Date()
                        } else if let startTime = self.feedbackStartTimes[primary], Date().timeIntervalSince(startTime) > self.frustrationThreshold {
                            // 5ì´ˆ ì´ìƒ ë™ì¼ í”¼ë“œë°± -> ë‚œì´ë„ ì™„í™”
                            if self.frustrationMultiplier == 1.0 { // ì•„ì§ ì™„í™” ì•ˆ ëœ ìƒíƒœë©´
                                print("ğŸ˜¤ ì¢Œì ˆ ê°ì§€! ë‚œì´ë„ ì™„í™” (Thresholds relax 1.2x)")
                                self.frustrationMultiplier = 1.2
                            }
                        }
                    }

                    for category in gateFeedbacks.map({ $0.category }) {
                        if self.feedbackHistory[category]! >= self.historyThreshold {
                            if let fb = gateFeedbacks.first(where: { $0.category == category }) {
                                stableFeedback.append(fb)
                            }
                        }
                    }
                    
                    // ì‚¬ë¼ì§„ ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™”
                    let currentCategories = Set(gateFeedbacks.map { $0.category })
                    for category in self.feedbackHistory.keys {
                        if !currentCategories.contains(category) {
                            self.feedbackHistory[category] = 0
                            // í•´ê²°ëœ í”¼ë“œë°±ì˜ íƒ€ì´ë¨¸ë„ ì œê±°
                            // (ì •í™•íˆ ë§¤í•‘í•˜ê¸° ì–´ë ¤ìš°ë©´ ì „ì²´ ë¦¬ì…‹í•˜ì§€ ì•Šê³  ìœ ì§€í•˜ë‹¤ê°€ ì£¼ìš” í”¼ë“œë°± ë³€ê²½ ì‹œ ì²˜ë¦¬ë¨)
                        }
                    }

                    print("ğŸ¯ v1.5 Gate: \(eval.passedCount)/5 í†µê³¼, ì ìˆ˜: \(String(format: "%.0f%%", Double(eval.overallScore) * 100))")
                }
            } else {
                // ğŸ”§ DEBUG: cachedReference nil
                print("â­ï¸ Gate í‰ê°€ ìŠ¤í‚µ: cachedReference nil (ë ˆí¼ëŸ°ìŠ¤ ìºì‹œ ëŒ€ê¸° ì¤‘)")
            }

            // ì™„ë²½ ìƒíƒœ ê°ì§€ (Gate System ê¸°ì¤€)
            let isCurrentlyPerfect = evaluation?.allPassed ?? false
            let score = evaluation.map { Double($0.overallScore) } ?? 0.0

            // ì™„ë£Œëœ í”¼ë“œë°± ê°ì§€ (íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì ìš©)
            let currentFeedbackIds = Set(stableFeedback.map { $0.id })
            let disappeared = self.previousFeedbackIds.subtracting(currentFeedbackIds)

            var completedToAdd: [CompletedFeedback] = []

            // ì‚¬ë¼ì§„ í”¼ë“œë°±ì˜ ì—°ì† íšŸìˆ˜ ì¶”ì 
            for disappearedId in disappeared {
                self.disappearedFeedbackHistory[disappearedId, default: 0] += 1

                // 5ë²ˆ ì—°ì† ì‚¬ë¼ì§€ë©´ ì™„ë£Œë¡œ íŒë‹¨
                if self.disappearedFeedbackHistory[disappearedId]! >= self.disappearedThreshold {
                    if let completedItem = self.instantFeedback.first(where: { $0.id == disappearedId }) {
                        let completed = CompletedFeedback(item: completedItem, completedAt: Date())
                        completedToAdd.append(completed)
                    }
                    // ì™„ë£Œ ì²˜ë¦¬ í›„ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
                    self.disappearedFeedbackHistory[disappearedId] = 0
                }
            }

            // ë‹¤ì‹œ ë‚˜íƒ€ë‚œ í”¼ë“œë°±ì€ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            for (feedbackId, _) in self.disappearedFeedbackHistory {
                if currentFeedbackIds.contains(feedbackId) {
                    self.disappearedFeedbackHistory[feedbackId] = 0
                }
            }

            // ì¹´í…Œê³ ë¦¬ë³„ ìƒíƒœ ê³„ì‚°
            let categoryStatuses = self.calculateCategoryStatuses(from: stableFeedback)

            // ë©”ì¸ ìŠ¤ë ˆë“œë¡œ UI ì—…ë°ì´íŠ¸ë§Œ ì „ë‹¬
            DispatchQueue.main.async { [weak self] in
                guard let self = self else { return }
                
                var newState = self.state

                // âœ… Phase 1 ìµœì í™”: ì¡°ê±´ ì—†ì´ í• ë‹¹ (Equatableì´ ë§ˆì§€ë§‰ì— ë¹„êµ)
                if let eval = evaluation {
                    newState.gateEvaluation = eval
                    newState.v15Feedback = eval.primaryFeedback
                    // ğŸ†• ìƒ·íƒ€ì… ë””ë²„ê·¸ ì •ë³´ ì „ë‹¬
                    newState.currentShotDebugInfo = eval.gate1.debugInfo

                    // ğŸ” ë””ë²„ê·¸: UIë¡œ ì „ë‹¬ë˜ëŠ” ê°’ í™•ì¸
                    print("ğŸ” [RealtimeAnalyzer] currentShotDebugInfo ì„¤ì •: \(eval.gate1.debugInfo ?? "nil")")
                }

                if let unified = unifiedFeedback {
                    newState.unifiedFeedback = unified
                }

                // ğŸ†• SimpleRealTimeGuide ê²°ê³¼ ì„¤ì •
                newState.simpleGuide = simpleGuideResult

                // ğŸ†• ActiveFeedback ê´€ë¦¬ (ì•ˆì •ì ì¸ í”¼ë“œë°± í‘œì‹œ)
                newState.activeFeedback = self.updateActiveFeedback(
                    currentActive: newState.activeFeedback,
                    newEvaluation: evaluation,
                    newUnified: unifiedFeedback
                )

                newState.instantFeedback = stableFeedback
                newState.perfectScore = score  // ì¡°ê±´ ì œê±° (Equatableì´ ì•Œì•„ì„œ ë¹„êµ)
                newState.categoryStatuses = categoryStatuses

                // ğŸ†• Phase 2: Temporal Lock Logic (State Machine)
                var currentProgress: Float = 0.0

                // ğŸ†• SimpleGuide ê¸°ë°˜ ì™„ë²½ ìƒíƒœ íŒë‹¨ (GateSystem ëŒ€ì²´)
                let isSimpleGuidePerfect = simpleGuideResult.guide == .perfect

                if isSimpleGuidePerfect {
                    switch self.stabilityState {
                    case .idle:
                        // ì´ì œ ë§‰ ì™„ë²½í•´ì§ -> íƒ€ì´ë¨¸ ì‹œì‘
                        self.stabilityState = .arming(startedAt: Date())
                        currentProgress = 0.0
                        
                    case .arming(let startedAt):
                        // ìœ ì§€ ì¤‘ -> ì‹œê°„ ê³„ì‚°
                        let elapsed = Date().timeIntervalSince(startedAt)
                        currentProgress = Float(min(elapsed / self.lockDuration, 1.0))
                        
                        if elapsed >= self.lockDuration {
                            self.stabilityState = .locked
                            currentProgress = 1.0
                            // ğŸ“³ Haptic Logic could go here (Triggered once)
                        }
                        
                    case .locked:
                        // ì´ë¯¸ ì ê¹€ -> ìœ ì§€
                        currentProgress = 1.0
                    }
                } else {
                    // ì¡°ê±´ ê¹¨ì§ -> ì¦‰ì‹œ ë¦¬ì…‹
                    self.stabilityState = .idle
                    currentProgress = 0.0
                }

                newState.stabilityProgress = currentProgress
                newState.isPerfect = (self.stabilityState == .locked)

                // ì™„ë£Œëœ í”¼ë“œë°±: ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ ì—…ë°ì´íŠ¸
                var updatedCompletedFeedbacks = newState.completedFeedbacks
                
                // 1. ìƒˆë¡œ ì™„ë£Œëœ í•­ëª© ì¶”ê°€
                if !completedToAdd.isEmpty {
                    updatedCompletedFeedbacks.append(contentsOf: completedToAdd)
                }
                
                // 2. ë§Œì•½ í˜„ì¬ ë‹¤ì‹œ ë°œìƒí•œ í”¼ë“œë°±ì´ ìˆë‹¤ë©´, ì™„ë£Œ ëª©ë¡ì—ì„œ ì œê±° (User Request: ë‹¤ì‹œ í”¼ë“œë°± ì‹œì‘)
                // í˜„ì¬ í™œì„± í”¼ë“œë°± ID ëª©ë¡
                let activeIds = Set(stableFeedback.map { $0.id })
                if !activeIds.isEmpty {
                    updatedCompletedFeedbacks.removeAll { completed in
                        // ì™„ë£Œëœ í•­ëª©ì˜ IDê°€ í˜„ì¬ í™œì„± ëª©ë¡ì— ìˆë‹¤ë©´ ì œê±° (ë‹¤ì‹œ ë¬¸ì œ ë°œìƒ)
                       activeIds.contains(completed.item.id)
                    }
                }
                
                updatedCompletedFeedbacks.removeAll { !$0.shouldDisplay }
                newState.completedFeedbacks = updatedCompletedFeedbacks

                // ì´ì „ í”¼ë“œë°± ì—…ë°ì´íŠ¸ (Internal state, not published)
                self.previousFeedbackIds = currentFeedbackIds

                // âœ… Final State Update: Equatable í•œ ë²ˆë§Œ ë¹„êµ
                if self.state != newState {
                    self.state = newState
                }
            }
        }
    }

    // MARK: - ğŸ†• Active Feedback Management (ì•ˆì •ì ì¸ í”¼ë“œë°± í‘œì‹œ)

    /// ActiveFeedback ì—…ë°ì´íŠ¸ - ë™ì¼ í”¼ë“œë°±ì€ ì§„í–‰ë¥ ë§Œ, ë‹¤ë¥¸ í”¼ë“œë°±ì€ ìµœì†Œ ì‹œê°„ í›„ êµì²´
    private func updateActiveFeedback(
        currentActive: ActiveFeedback?,
        newEvaluation: GateEvaluation?,
        newUnified: UnifiedFeedback?
    ) -> ActiveFeedback? {
        guard let eval = newEvaluation else {
            // í‰ê°€ ê²°ê³¼ ì—†ìŒ â†’ í™œì„± í”¼ë“œë°± ìœ ì§€ (í•´ê²° ì¤‘ì¼ ìˆ˜ ìˆìŒ)
            if var active = currentActive, active.shouldRemove {
                return nil  // í˜ì´ë“œì•„ì›ƒ ì™„ë£Œ
            }
            return currentActive
        }

        // ëª¨ë“  Gate í†µê³¼ â†’ í•´ê²° ì²˜ë¦¬
        if eval.allPassed {
            if var active = currentActive {
                if !active.isResolved {
                    active.updateProgress(1.0)  // 100%ë¡œ ì„¤ì •
                }
                if active.shouldRemove {
                    return nil  // í˜ì´ë“œì•„ì›ƒ ì™„ë£Œ
                }
                return active
            }
            return nil
        }

        // í˜„ì¬ ì‹¤íŒ¨í•œ Gate ì •ë³´ (ì—†ìœ¼ë©´ ëª¨ë‘ í†µê³¼)
        guard let newGateIndex = eval.currentFailedGate else {
            // ëª¨ë‘ í†µê³¼í–ˆì§€ë§Œ allPassed ì¡°ê±´ì—ì„œ ì•ˆ ê±¸ë¦° ê²½ìš°
            return currentActive
        }

        let newFeedbackType = extractFeedbackType(from: newUnified, gateIndex: newGateIndex)
        let newMessage = newUnified?.mainMessage ?? eval.primaryFeedback
        let newProgress = calculateProgress(for: eval, gateIndex: newGateIndex)

        // í˜„ì¬ í™œì„± í”¼ë“œë°±ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        guard var active = currentActive else {
            return ActiveFeedback(
                gateIndex: newGateIndex,
                feedbackType: newFeedbackType,
                message: newMessage,
                initialProgress: newProgress
            )
        }

        // ê°™ì€ í”¼ë“œë°±ì´ë©´ ì§„í–‰ë¥ ë§Œ ì—…ë°ì´íŠ¸
        if active.gateIndex == newGateIndex && active.feedbackType == newFeedbackType {
            active.updateProgress(newProgress)
            return active
        }

        // ë‹¤ë¥¸ í”¼ë“œë°±ì´ì§€ë§Œ ìµœì†Œ í‘œì‹œ ì‹œê°„ì´ ì•ˆ ì§€ë‚¬ìœ¼ë©´ ìœ ì§€
        if !active.hasMinDisplayTimePassed && !active.isResolved {
            // ê¸°ì¡´ í”¼ë“œë°± ì§„í–‰ë¥ ì€ ìœ ì§€í•˜ë˜ ë‚´ë¶€ì ìœ¼ë¡œ ìƒˆ í”¼ë“œë°± ì¶”ì 
            return active
        }

        // ìƒˆ í”¼ë“œë°±ìœ¼ë¡œ êµì²´
        return ActiveFeedback(
            gateIndex: newGateIndex,
            feedbackType: newFeedbackType,
            message: newMessage,
            initialProgress: newProgress
        )
    }

    /// UnifiedFeedback ë˜ëŠ” gateIndexì—ì„œ í”¼ë“œë°± íƒ€ì… ì¶”ì¶œ
    private func extractFeedbackType(from unified: UnifiedFeedback?, gateIndex: Int) -> String {
        if let unified = unified {
            return unified.primaryAction.rawValue
        }
        // Gateë³„ ê¸°ë³¸ íƒ€ì…
        switch gateIndex {
        case 0: return "aspect_ratio"
        case 1: return "framing"
        case 2: return "position"
        case 3: return "compression"
        case 4: return "pose"
        default: return "unknown"
        }
    }

    /// Gateë³„ ì§„í–‰ë¥  ê³„ì‚° (0.0 ~ 1.0) - GateResult.score ê¸°ë°˜
    private func calculateProgress(for eval: GateEvaluation, gateIndex: Int) -> CGFloat {
        // ê° Gateì˜ scoreë¥¼ ì§ì ‘ ì‚¬ìš© (0.0 ~ 1.0)
        switch gateIndex {
        case 0: return eval.gate0.score
        case 1: return eval.gate1.score
        case 2: return eval.gate2.score
        case 3: return eval.gate3.score
        case 4: return eval.gate4.score
        default: return 0.0
        }
    }

    // MARK: - Category Status Calculation

    /// ì¹´í…Œê³ ë¦¬ë³„ ìƒíƒœ ê³„ì‚°
    private func calculateCategoryStatuses(from feedbacks: [FeedbackItem]) -> [CategoryStatus] {
        // ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ìƒíƒœ ìƒì„±
        var statusMap: [FeedbackCategory: CategoryStatus] = [:]

        // ê° ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™” (ëª¨ë‘ ë§Œì¡± ìƒíƒœë¡œ ì‹œì‘)
        for category in FeedbackCategory.allCases {
            statusMap[category] = CategoryStatus(
                category: category,
                isSatisfied: true,
                activeFeedbacks: []
            )
        }

        // í”¼ë“œë°±ì´ ìˆëŠ” ì¹´í…Œê³ ë¦¬ëŠ” ë¶ˆë§Œì¡± ìƒíƒœë¡œ ë³€ê²½
        for feedback in feedbacks {
            if let category = FeedbackCategory.from(categoryString: feedback.category) {
                var activeFeedbacks = statusMap[category]?.activeFeedbacks ?? []
                activeFeedbacks.append(feedback)

                statusMap[category] = CategoryStatus(
                    category: category,
                    isSatisfied: false,
                    activeFeedbacks: activeFeedbacks.sorted { $0.priority < $1.priority }
                )
            }
        }

        // ìš°ì„ ìˆœìœ„ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        return Array(statusMap.values).sorted { $0.priority < $1.priority }
    }

    // MARK: - ë””ë²„ê·¸ í—¬í¼
    private func saveDebugImage(_ image: UIImage, reason: String) {
        guard let data = image.jpegData(compressionQuality: 0.8) else { return }

        let timestamp = DateFormatter.localizedString(from: Date(), dateStyle: .none, timeStyle: .medium)
            .replacingOccurrences(of: ":", with: "-")
        let filename = "debug_\(reason)_\(timestamp).jpg"

        if let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first {
            let fileURL = documentsPath.appendingPathComponent(filename)
            try? data.write(to: fileURL)
            print("ğŸ” ë””ë²„ê·¸ ì´ë¯¸ì§€ ì €ì¥: \(fileURL.path)")
        }
    }
}
