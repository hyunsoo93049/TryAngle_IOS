import Foundation
import UIKit
import CoreGraphics
import Accelerate
import simd
import Vision
import CoreML

// MARK: - RTMPose ê²°ê³¼ êµ¬ì¡°ì²´
struct RTMPoseResult {
    let keypoints: [(point: CGPoint, confidence: Float)]  // 133ê°œ í‚¤í¬ì¸íŠ¸
    let boundingBox: CGRect?  // ì¸ë¬¼ ê²€ì¶œ ë°•ìŠ¤
}

// MARK: - RTMPose Runner (CoreML + ONNX Runtime)
// ì—­í• : YOLO11n(ì‚¬ëŒ ê²€ì¶œ, CoreML) + RTMPose(133ê°œ í‚¤í¬ì¸íŠ¸, ONNX) ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” í•µì‹¬ ëŸ¬ë„ˆì…ë‹ˆë‹¤.
//       ì‹±ê¸€í†¤ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ì•± ì „ì²´ì—ì„œ í•˜ë‚˜ì˜ ì¸ìŠ¤í„´ìŠ¤ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
class RTMPoseRunner {

    // MARK: - Singleton (ì§€ì—° ì´ˆê¸°í™”)
    private static var _shared: RTMPoseRunner?
    private static let initQueue = DispatchQueue(label: "rtmpose.init", qos: .userInitiated)
    private static var isInitializing = false

    static var shared: RTMPoseRunner? {
        if let instance = _shared { return instance }

        // ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì•„ì§ ì´ˆê¸°í™” ì•ˆë¨ â†’ nil ë°˜í™˜ (ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„)
        initializeInBackground()
        return _shared
    }

    /// ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ì´ˆê¸°í™” (ì•± ì‹œì‘ ì‹œ í˜¸ì¶œ)
    static func initializeInBackground(completion: (() -> Void)? = nil) {
        guard _shared == nil && !isInitializing else {
            completion?()
            return
        }

        isInitializing = true

        initQueue.async {
            print("ğŸš€ RTMPoseRunner ë°±ê·¸ë¼ìš´ë“œ ì´ˆê¸°í™” ì‹œì‘")
            _shared = RTMPoseRunner()
            isInitializing = false
            print("âœ… RTMPoseRunner ë°±ê·¸ë¼ìš´ë“œ ì´ˆê¸°í™” ì™„ë£Œ")

            DispatchQueue.main.async {
                completion?()
            }
        }
    }

    // YOLO11n CoreML (ì‚¬ëŒ ê²€ì¶œ)
    private var yoloModel: VNCoreMLModel?

    // RTMPose ONNX (í¬ì¦ˆ ì¶”ì •)
    private var poseSession: ORTSession?
    private var env: ORTEnv?

    // ëª¨ë¸ ê²½ë¡œ
    private let poseModelPath: String

    // ëª¨ë¸ ì…ë ¥ í¬ê¸°
    private let detectorInputSize = CGSize(width: 640, height: 640)
    private let poseInputSize = CGSize(width: 192, height: 256)

    private init?() {
        // ğŸ”¥ ì´ initì€ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œë§Œ í˜¸ì¶œë¨
        print("ğŸš€ RTMPoseRunner init() ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)")

        // YOLO11n CoreML ëª¨ë¸ (ì‚¬ëŒ ê²€ì¶œ) - ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
        guard let yoloURL = Bundle.main.url(forResource: "YOLO11nDetector", withExtension: "mlmodelc") else {
            print("âŒ YOLO11nDetector.mlmodelc íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return nil
        }

        // RTMPose ONNX ëª¨ë¸ (í¬ì¦ˆ ì¶”ì •)
        guard let poseURL = Bundle.main.url(forResource: "rtmpose_int8", withExtension: "onnx") else {
            print("âŒ rtmpose_int8.onnx íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return nil
        }

        poseModelPath = poseURL.path

        print("âœ… ëª¨ë¸ ê²½ë¡œ í™•ì¸:")
        print("   Detector (YOLO11n CoreML): \(yoloURL.path)")
        print("   Pose (RTMPose ONNX): \(poseModelPath)")

        // CoreML ëª¨ë¸ ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œ)
        setupCoreMLDetector(yoloURL: yoloURL)

        // ONNX Runtime ì´ˆê¸°í™” (ë°±ê·¸ë¼ìš´ë“œ)
        setupONNXRuntime()
    }

    deinit {
        print("ğŸ—‘ï¸ RTMPoseRunner deinit")
    }

    // MARK: - CoreML Detector ì´ˆê¸°í™” (YOLO11n)
    private func setupCoreMLDetector(yoloURL: URL) {
        print("ğŸ”§ YOLO11n CoreML ì´ˆê¸°í™” ì‹œì‘...")
        logMemory("YOLO11n ë¡œë“œ ì „")

        do {
            // CoreML ëª¨ë¸ ë¡œë“œ (ì»´íŒŒì¼)
            let config = MLModelConfiguration()
            config.computeUnits = .all  // Neural Engine + GPU + CPU ìë™ ì„ íƒ

            let mlModel = try MLModel(contentsOf: yoloURL, configuration: config)
            yoloModel = try VNCoreMLModel(for: mlModel)
            print("âœ… YOLO11n CoreML ë¡œë“œ ì„±ê³µ (Neural Engine ê°€ì†)")
            logMemory("YOLO11n ë¡œë“œ í›„")
        } catch {
            print("âŒ YOLO11n CoreML ë¡œë“œ ì‹¤íŒ¨: \(error)")
            yoloModel = nil
        }
    }

    // MARK: - ONNX Runtime ì´ˆê¸°í™” (RTMPoseë§Œ)
    private func setupONNXRuntime() {
        print("ğŸ”§ ONNX Runtime ì´ˆê¸°í™” ì‹œì‘ (RTMPose)...")

        do {
            // 1. Environment ìƒì„±
            env = try ORTEnv(loggingLevel: ORTLoggingLevel.warning)
            print("âœ… Environment ìƒì„± ì„±ê³µ")

            // 2. RTMPoseìš© Session Options (CoreML GPU ê°€ì†)
            let poseOptions = try ORTSessionOptions()

            // ğŸ”¥ CoreML Execution Provider í™œì„±í™” (GPU ê°€ì†)
            do {
                try poseOptions.appendCoreMLExecutionProvider()
                print("âœ… RTMPose: CoreML GPU ê°€ì† í™œì„±í™”")
            } catch {
                print("âš ï¸ RTMPose CoreML í™œì„±í™” ì‹¤íŒ¨, CPU í´ë°±: \(error)")
            }

            // ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • (ìµœëŒ€ ì„±ëŠ¥)
            try poseOptions.setIntraOpNumThreads(6)
            try poseOptions.setGraphOptimizationLevel(.all)

            // 3. RTMPose ì„¸ì…˜ ìƒì„±
            logMemory("RTMPose ë¡œë“œ ì „")

            print("ğŸ“¦ Pose ëª¨ë¸ ë¡œë”© ì¤‘... (\(poseModelPath))")
            poseSession = try ORTSession(env: env!, modelPath: poseModelPath, sessionOptions: poseOptions)
            print("âœ… RTMPose ë¡œë“œ ì„±ê³µ (CoreML GPU)")
            logMemory("RTMPose ë¡œë“œ í›„")

            print("ğŸ”§ ONNX Runtime ì´ˆê¸°í™” ì™„ë£Œ")

        } catch {
            print("âŒ ONNX Runtime ì´ˆê¸°í™” ì‹¤íŒ¨: \(error)")
            env = nil
            poseSession = nil
        }
    }

    // MARK: - YOLO11n CoreMLë¡œ ì‚¬ëŒ ê²€ì¶œ (BBoxë§Œ í•„ìš”í•  ë•Œ)
    func detectPersonBBox(from image: UIImage) -> CGRect? {
        guard let yoloModel = yoloModel else {
            print("âŒ YOLO11n CoreML ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return nil
        }

        return detectPersonWithCoreML(from: image, model: yoloModel)
    }

    // MARK: - YOLO11n CoreMLë¡œ ëª¨ë“  ì‚¬ëŒ ê²€ì¶œ (ë©€í‹° person)
    func detectAllPersonBBoxes(from image: UIImage) -> [CGRect] {
        guard let yoloModel = yoloModel else {
            print("âŒ YOLO11n CoreML ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return []
        }

        return detectAllPersonsWithCoreML(from: image, model: yoloModel)
    }

    // MARK: - ì„¸ì…˜ ìƒíƒœ í™•ì¸
    var isReady: Bool {
        return yoloModel != nil && poseSession != nil && env != nil
    }

    // MARK: - í¬ì¦ˆ ì¶”ì •
    func detectPose(from image: UIImage) -> RTMPoseResult? {
        guard let yoloModel = yoloModel,
              let poseSession = poseSession,
              let env = env else {
            print("âŒ RTMPose ì„¸ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return nil
        }

        // 1. YOLO11n CoreMLë¡œ ì‚¬ëŒ ê²€ì¶œ
        guard let detectedBox = detectPersonWithCoreML(from: image, model: yoloModel) else {
            // ğŸ”§ ìˆ˜ì •: ì‚¬ëŒ ê²€ì¶œ ì‹¤íŒ¨ ì‹œ RTMPose ì‹¤í–‰ ì•ˆ í•¨
            print("âš ï¸ YOLO11n: ì‚¬ëŒ ê²€ì¶œ ì•ˆë¨ â†’ í¬ì¦ˆ ì¶”ì • ê±´ë„ˆëœ€")
            return nil
        }

        print("âœ… YOLO11n: ì‚¬ëŒ ê²€ì¶œ ì„±ê³µ - \(detectedBox)")

        // 2. ê²€ì¶œëœ ì˜ì—­ìœ¼ë¡œ í¬ì¦ˆ ì¶”ì •
        let keypoints = estimatePose(from: image, boundingBox: detectedBox, using: poseSession, env: env)

        if let keypoints = keypoints {
            print("âœ… RTMPose: \(keypoints.count)ê°œ í‚¤í¬ì¸íŠ¸ ê²€ì¶œ ì„±ê³µ")
        } else {
            print("âŒ RTMPose: í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨")
        }

        return keypoints.map { RTMPoseResult(keypoints: $0, boundingBox: detectedBox) }
    }

    // MARK: - YOLO11n CoreML ì‚¬ëŒ ê²€ì¶œ (ë‹¨ì¼)
    private static let visionQueue = DispatchQueue(label: "yolo11n.vision", qos: .userInitiated)

    private func detectPersonWithCoreML(from image: UIImage, model: VNCoreMLModel) -> CGRect? {
        guard let cgImage = image.cgImage else { return nil }

        var resultBox: CGRect?
        let semaphore = DispatchSemaphore(value: 0)
        let imageWidth = cgImage.width
        let imageHeight = cgImage.height

        // ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ Vision ìš”ì²­ ì‹¤í–‰
        Self.visionQueue.async {
            // ğŸ”¥ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€: Vision ìš”ì²­ ê°ì²´ ì¦‰ì‹œ í•´ì œ
            autoreleasepool {
                let request = VNCoreMLRequest(model: model) { request, error in
                    if let error = error {
                        print("âŒ YOLO11n ì¶”ë¡  ì˜¤ë¥˜: \(error)")
                        semaphore.signal()
                        return
                    }

                    // VNRecognizedObjectObservationìœ¼ë¡œ ê²°ê³¼ íŒŒì‹±
                    guard let results = request.results as? [VNRecognizedObjectObservation] else {
                        print("âš ï¸ YOLO11n: ê²°ê³¼ í˜•ì‹ ë¶ˆì¼ì¹˜")
                        semaphore.signal()
                        return
                    }

                    // person í´ë˜ìŠ¤ë§Œ í•„í„°ë§í•˜ê³  ê°€ì¥ ë†’ì€ confidence ì„ íƒ
                    var bestBox: CGRect?
                    var bestConfidence: Float = 0.3  // ìµœì†Œ ì„ê³„ê°’

                    for observation in results {
                        // person í´ë˜ìŠ¤ í™•ì¸ (COCO í´ë˜ìŠ¤ 0)
                        if let topLabel = observation.labels.first,
                           topLabel.identifier == "person" || topLabel.identifier == "0",
                           topLabel.confidence > bestConfidence {
                            bestConfidence = topLabel.confidence
                            // Vision ì¢Œí‘œê³„ (ì¢Œí•˜ë‹¨ ì›ì ) â†’ UIKit ì¢Œí‘œê³„ ë³€í™˜
                            let bbox = observation.boundingBox
                            bestBox = CGRect(
                                x: bbox.minX * CGFloat(imageWidth),
                                y: (1 - bbox.maxY) * CGFloat(imageHeight),
                                width: bbox.width * CGFloat(imageWidth),
                                height: bbox.height * CGFloat(imageHeight)
                            )
                        }
                    }

                    resultBox = bestBox
                    semaphore.signal()
                }

                request.imageCropAndScaleOption = .scaleFill

                let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
                do {
                    try handler.perform([request])
                } catch {
                    print("âŒ YOLO11n Vision ì‹¤í–‰ ì˜¤ë¥˜: \(error)")
                    semaphore.signal()
                }
            }
        }

        semaphore.wait()
        return resultBox
    }

    // MARK: - YOLO11n CoreML ëª¨ë“  ì‚¬ëŒ ê²€ì¶œ (ë©€í‹°)
    private func detectAllPersonsWithCoreML(from image: UIImage, model: VNCoreMLModel) -> [CGRect] {
        guard let cgImage = image.cgImage else { return [] }

        var resultBoxes: [CGRect] = []
        let semaphore = DispatchSemaphore(value: 0)
        let imageWidth = cgImage.width
        let imageHeight = cgImage.height

        // ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ Vision ìš”ì²­ ì‹¤í–‰
        Self.visionQueue.async {
            // ğŸ”¥ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€: Vision ìš”ì²­ ê°ì²´ ì¦‰ì‹œ í•´ì œ
            autoreleasepool {
                let request = VNCoreMLRequest(model: model) { request, error in
                    if let error = error {
                        print("âŒ YOLO11n ì¶”ë¡  ì˜¤ë¥˜: \(error)")
                        semaphore.signal()
                        return
                    }

                    guard let results = request.results as? [VNRecognizedObjectObservation] else {
                        semaphore.signal()
                        return
                    }

                    let threshold: Float = 0.3

                    for observation in results {
                        if let topLabel = observation.labels.first,
                           topLabel.identifier == "person" || topLabel.identifier == "0",
                           topLabel.confidence > threshold {
                            let bbox = observation.boundingBox
                            let box = CGRect(
                                x: bbox.minX * CGFloat(imageWidth),
                                y: (1 - bbox.maxY) * CGFloat(imageHeight),
                                width: bbox.width * CGFloat(imageWidth),
                                height: bbox.height * CGFloat(imageHeight)
                            )
                            resultBoxes.append(box)
                        }
                    }
                    semaphore.signal()
                }

                request.imageCropAndScaleOption = .scaleFill

                let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
                do {
                    try handler.perform([request])
                } catch {
                    print("âŒ YOLO11n Vision ì‹¤í–‰ ì˜¤ë¥˜: \(error)")
                    semaphore.signal()
                }
            }
        }

        semaphore.wait()
        return resultBoxes
    }

    // MARK: - RTMPose í¬ì¦ˆ ì¶”ì •
    private func estimatePose(from image: UIImage, boundingBox: CGRect, using session: ORTSession, env: ORTEnv) -> [(point: CGPoint, confidence: Float)]? {
        guard let cgImage = image.cgImage else { return nil }

        // ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ í¬ë¡­
        guard let croppedImage = cropImage(cgImage, rect: boundingBox) else { return nil }

        // 192x256ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        let inputSize = poseInputSize
        guard let resizedImage = resizeImage(croppedImage, targetSize: inputSize) else { return nil }

        // ì´ë¯¸ì§€ë¥¼ Float ë°°ì—´ë¡œ ë³€í™˜
        let pixelData = preprocessImage(resizedImage, size: inputSize)

        do {
            // ì…ë ¥ í…ì„œ ìƒì„± - [1, 3, 256, 192]
            let inputShape: [NSNumber] = [1, 3, NSNumber(value: Int(inputSize.height)), NSNumber(value: Int(inputSize.width))]
            let inputTensor = try ORTValue(
                tensorData: NSMutableData(data: pixelData),
                elementType: .float,
                shape: inputShape
            )

            // ì¶”ë¡  ì‹¤í–‰
            let outputs = try session.run(
                withInputs: ["input": inputTensor],
                outputNames: ["simcc_x", "simcc_y"],
                runOptions: nil
            )

            guard let simccX = outputs["simcc_x"],
                  let simccY = outputs["simcc_y"] else {
                print("âŒ RTMPose ì¶œë ¥(SimCC)ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return nil
            }

            // SimCC ì¶œë ¥ íŒŒì‹±í•˜ì—¬ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (133ê°œ)
            let imageSize = image.size
            return parseRTMPoseSimCCOutput(simccX: simccX, simccY: simccY, boundingBox: boundingBox, imageSize: imageSize)

        } catch {
            print("âŒ RTMPose ì¶”ë¡  ì˜¤ë¥˜: \(error)")
            return nil
        }
    }

    // MARK: - ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜ë“¤
    private func resizeImage(_ cgImage: CGImage, targetSize: CGSize) -> CGImage? {
        let width = Int(targetSize.width)
        let height = Int(targetSize.height)

        guard let context = CGContext(
            data: nil,
            width: width,
            height: height,
            bitsPerComponent: 8,
            bytesPerRow: width * 4,
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipLast.rawValue
        ) else { return nil }

        context.interpolationQuality = .high
        context.draw(cgImage, in: CGRect(origin: .zero, size: targetSize))
        return context.makeImage()
    }

    private func cropImage(_ cgImage: CGImage, rect: CGRect) -> CGImage? {
        // ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì¶©ë¶„íˆ í™•ì¥ (ì†ì´ í¬í•¨ë˜ë„ë¡ íŒ¨ë”© ì¦ê°€)
        // ğŸ”¥ ì† ì¸ì‹ ê°œì„ : íŒ¨ë”©ì„ 0.2ì—ì„œ 0.4ë¡œ ì¦ê°€
        let padding: CGFloat = 0.4  // 40% íŒ¨ë”©ìœ¼ë¡œ ì†ê¹Œì§€ í¬í•¨
        let expandedRect = CGRect(
            x: rect.minX - rect.width * padding,
            y: rect.minY - rect.height * padding,
            width: rect.width * (1 + 2 * padding),
            height: rect.height * (1 + 2 * padding)
        ).intersection(CGRect(x: 0, y: 0, width: cgImage.width, height: cgImage.height))

        return cgImage.cropping(to: expandedRect)
    }

    // ğŸ”¥ Accelerate ê¸°ë°˜ ê³ ì† ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    private func preprocessImage(_ cgImage: CGImage, size: CGSize) -> Data {
        let width = Int(size.width)
        let height = Int(size.height)
        let pixelCount = width * height

        var rawData = [UInt8](repeating: 0, count: pixelCount * 4)

        guard let context = CGContext(
            data: &rawData,
            width: width,
            height: height,
            bitsPerComponent: 8,
            bytesPerRow: width * 4,
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipLast.rawValue
        ) else {
            return Data()
        }

        context.draw(cgImage, in: CGRect(x: 0, y: 0, width: width, height: height))

        // ğŸ”¥ vDSPë¥¼ ì‚¬ìš©í•œ ë²¡í„°í™”ëœ ì •ê·œí™” (ImageNet í‰ê· /í‘œì¤€í¸ì°¨)
        var floatData = [Float](repeating: 0, count: pixelCount * 3)
        let mean: [Float] = [0.485, 0.456, 0.406]
        let std: [Float] = [0.229, 0.224, 0.225]

        // ê° ì±„ë„ë³„ ì²˜ë¦¬ (ë³‘ë ¬í™”)
        DispatchQueue.concurrentPerform(iterations: 3) { c in
            var channelData = [Float](repeating: 0, count: pixelCount)

            // RGBAì—ì„œ í•´ë‹¹ ì±„ë„ ì¶”ì¶œ (strideë¡œ ì ‘ê·¼)
            for i in 0..<pixelCount {
                channelData[i] = Float(rawData[i * 4 + c])
            }

            // vDSP: /255.0 ì •ê·œí™”
            var scale: Float = 1.0 / 255.0
            vDSP_vsmul(channelData, 1, &scale, &channelData, 1, vDSP_Length(pixelCount))

            // vDSP: (x - mean) ë¹¼ê¸°
            var negMean = -mean[c]
            vDSP_vsadd(channelData, 1, &negMean, &channelData, 1, vDSP_Length(pixelCount))

            // vDSP: / std ë‚˜ëˆ„ê¸°
            var invStd = 1.0 / std[c]
            vDSP_vsmul(channelData, 1, &invStd, &channelData, 1, vDSP_Length(pixelCount))

            // CHW í¬ë§·ìœ¼ë¡œ ë³µì‚¬
            let offset = c * pixelCount
            for i in 0..<pixelCount {
                floatData[offset + i] = channelData[i]
            }
        }

        return Data(bytes: &floatData, count: floatData.count * MemoryLayout<Float>.size)
    }

    // MARK: - RTMPose SimCC ì¶œë ¥ íŒŒì‹±
    private func parseRTMPoseSimCCOutput(simccX: ORTValue, simccY: ORTValue, boundingBox: CGRect, imageSize: CGSize) -> [(point: CGPoint, confidence: Float)]? {
        // SimCC ì¶œë ¥ í˜•ì‹:
        // simcc_x: [1, num_keypoints, 384] - x ì¢Œí‘œ í™•ë¥  ë¶„í¬
        // simcc_y: [1, num_keypoints, 512] - y ì¢Œí‘œ í™•ë¥  ë¶„í¬

        guard let xData = try? simccX.tensorData() as NSData,
              let yData = try? simccY.tensorData() as NSData else { return nil }
        guard let xShape = try? simccX.tensorTypeAndShapeInfo().shape,
              let yShape = try? simccY.tensorTypeAndShapeInfo().shape else { return nil }

        let numKeypoints = xShape[1].intValue
        let xBins = xShape[2].intValue  // 384
        let yBins = yShape[2].intValue  // 512

        if numKeypoints != 133 {
            print("âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ í‚¤í¬ì¸íŠ¸ ìˆ˜: \(numKeypoints)")
            return nil
        }

        var keypoints: [(point: CGPoint, confidence: Float)] = []
        let xPointer = xData.bytes.bindMemory(to: Float.self, capacity: xData.length / MemoryLayout<Float>.size)
        let yPointer = yData.bytes.bindMemory(to: Float.self, capacity: yData.length / MemoryLayout<Float>.size)

        for i in 0..<numKeypoints {
            // x ì¢Œí‘œ: argmax ì°¾ê¸°
            let xOffset = i * xBins
            var maxXIdx = 0
            var maxXVal: Float = -Float.infinity
            for j in 0..<xBins {
                let val = xPointer[xOffset + j]
                if val > maxXVal {
                    maxXVal = val
                    maxXIdx = j
                }
            }

            // y ì¢Œí‘œ: argmax ì°¾ê¸°
            let yOffset = i * yBins
            var maxYIdx = 0
            var maxYVal: Float = -Float.infinity
            for j in 0..<yBins {
                let val = yPointer[yOffset + j]
                if val > maxYVal {
                    maxYVal = val
                    maxYIdx = j
                }
            }

            // SimCC ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            // 384 bins -> 192 pixels, 512 bins -> 256 pixels (ê°ê° 2ë°° í•´ìƒë„)
            let xNorm = CGFloat(maxXIdx) / CGFloat(xBins) * poseInputSize.width
            let yNorm = CGFloat(maxYIdx) / CGFloat(yBins) * poseInputSize.height

            // ë°”ìš´ë”© ë°•ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜ í›„ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì •ê·œí™” (0.0~1.0)
            let point = CGPoint(
                x: (boundingBox.minX + (xNorm / poseInputSize.width) * boundingBox.width) / imageSize.width,
                y: (boundingBox.minY + (yNorm / poseInputSize.height) * boundingBox.height) / imageSize.height
            )

            // ì‹ ë¢°ë„: ë‘ í™•ë¥ ì˜ í‰ê· 
            let confidence = (maxXVal + maxYVal) / 2.0

            keypoints.append((point: point, confidence: confidence))

            // ğŸ” ì† í‚¤í¬ì¸íŠ¸ ë””ë²„ê·¸ (91-132ë²ˆ)
            if i >= 91 && i <= 132 {
                if confidence < 0.3 {
                    let handName = i <= 111 ? "ì™¼ì†" : "ì˜¤ë¥¸ì†"
                    let keypointIndex = i <= 111 ? i - 91 : i - 112
                    if keypointIndex % 5 == 0 {  // 5ê°œë§ˆë‹¤ í•œ ë²ˆë§Œ ë¡œê·¸
                        print("âš ï¸ \(handName) í‚¤í¬ì¸íŠ¸ \(keypointIndex): ì‹ ë¢°ë„ ë‚®ìŒ (\(String(format: "%.2f", confidence)))")
                    }
                }
            }
        }

        // ì† í‚¤í¬ì¸íŠ¸ ìš”ì•½ í†µê³„
        let leftHandConfidences = (91...111).compactMap { keypoints[$0].confidence }
        let rightHandConfidences = (112...132).compactMap { keypoints[$0].confidence }

        let leftHandAvg = leftHandConfidences.reduce(0, +) / Float(leftHandConfidences.count)
        let rightHandAvg = rightHandConfidences.reduce(0, +) / Float(rightHandConfidences.count)

        if leftHandAvg < 0.5 || rightHandAvg < 0.5 {
            print("ğŸ“Š ì† ì¸ì‹ í‰ê·  ì‹ ë¢°ë„ - ì™¼ì†: \(String(format: "%.2f", leftHandAvg)), ì˜¤ë¥¸ì†: \(String(format: "%.2f", rightHandAvg))")
            if leftHandAvg < 0.3 || rightHandAvg < 0.3 {
                print("ğŸ’¡ ì†ì´ í™”ë©´ì—ì„œ ì˜ë ¸ê±°ë‚˜ ê°€ë ¤ì¡Œì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ì‹ ì²´ê°€ í”„ë ˆì„ ì•ˆì— ë“¤ì–´ì˜¤ë„ë¡ ì¡°ì •í•´ë³´ì„¸ìš”.")
            }
        }

        return keypoints
    }
}
