import Foundation
import UIKit
import CoreGraphics
import Accelerate
import simd
import Vision
import CoreML

// MARK: - RTMPose ê²°ê³¼ êµ¬ì¡°ì²´
struct RTMPoseResult {
    let keypoints: [(point: CGPoint, confidence: Float)]  // 133ê°œ í‚¤í¬ì¸íŠ¸
    let boundingBox: CGRect?  // ì¸ë¬¼ ê²€ì¶œ ë°•ìŠ¤
}

// MARK: - RTMPose Runner (CoreML + ONNX Runtime)
// ì—­í• : YOLO11n(ì‚¬ëŒ ê²€ì¶œ, CoreML) + RTMPose(133ê°œ í‚¤í¬ì¸íŠ¸, ONNX) ëª¨ë¸ì„ ì‹¤í–‰í•˜ëŠ” í•µì‹¬ ëŸ¬ë„ˆì…ë‹ˆë‹¤.
//       ì‹±ê¸€í†¤ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ì•± ì „ì²´ì—ì„œ í•˜ë‚˜ì˜ ì¸ìŠ¤í„´ìŠ¤ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
class RTMPoseRunner {

    // MARK: - Singleton (ì§€ì—° ì´ˆê¸°í™”)
    private static var _shared: RTMPoseRunner?
    private static let initQueue = DispatchQueue(label: "rtmpose.init", qos: .userInitiated)
    private static var isInitializing = false

    static var shared: RTMPoseRunner? {
        if let instance = _shared { return instance }

        // ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì•„ì§ ì´ˆê¸°í™” ì•ˆë¨ â†’ nil ë°˜í™˜ (ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„)
        initializeInBackground()
        return _shared
    }

    /// ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ì´ˆê¸°í™” (ì•± ì‹œì‘ ì‹œ í˜¸ì¶œ)
    static func initializeInBackground(completion: (() -> Void)? = nil) {
        guard _shared == nil && !isInitializing else {
            completion?()
            return
        }

        isInitializing = true

        initQueue.async {
            
            
            logInfo("ì´ˆê¸°í™” ì‹œì‘", category:"RTMPose")
            _shared = RTMPoseRunner()
            isInitializing = false
            logInfo("ì´ˆê¸°í™” ì™„ë£Œ", category:"RTMPose")

            DispatchQueue.main.async {
                completion?()
            }
        }
    }

    // YOLO11n CoreML (ì‚¬ëŒ ê²€ì¶œ)
    private var yoloModel: VNCoreMLModel?

    // RTMPose ONNX (í¬ì¦ˆ ì¶”ì •)
    private var poseSession: ORTSession?
    private var env: ORTEnv?

    // ëª¨ë¸ ê²½ë¡œ
    private let poseModelPath: String

    // ëª¨ë¸ ì…ë ¥ í¬ê¸°
    private let detectorInputSize = CGSize(width: 640, height: 640)
    private let poseInputSize = CGSize(width: 192, height: 256)

    private init?() {
        // ğŸ”¥ ì´ initì€ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œë§Œ í˜¸ì¶œë¨
        logInfo("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ", category : "RTMPose")
        
        // RTMPose ONNX ëª¨ë¸ (í¬ì¦ˆ ì¶”ì •)
        // ğŸ”§ ìˆ˜ì •: Bundle(for:) ì‚¬ìš©í•˜ì—¬ ì˜¬ë°”ë¥¸ ë²ˆë“¤ì—ì„œ ì°¾ê¸°
        let poseURL: URL? = Bundle(for: RTMPoseRunner.self).url(forResource: "rtmpose_int8", withExtension: "onnx")
            ?? Bundle.main.url(forResource: "rtmpose_int8", withExtension: "onnx")

        guard let poseURL = poseURL else {
            logError("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - error : rtmpose_int8.onnx íŒŒì¼ ì—†ìŒ", category: "RTMPose")
            return nil
        }

        poseModelPath = poseURL.path

        

        // ğŸ”§ ìˆ˜ì •: Xcode ìë™ ìƒì„± YOLO11nDetector í´ë˜ìŠ¤ ì‚¬ìš©
        setupCoreMLDetector()

        // ONNX Runtime ì´ˆê¸°í™” (ë°±ê·¸ë¼ìš´ë“œ)
        setupONNXRuntime()
    }

    deinit {
        logDebug("deinit - RTMPoseRunner ë©”ëª¨ë¦¬ í•´ì œ - initë°˜í™˜", category: "RTMPose")
    }

    // MARK: - CoreML Detector ì´ˆê¸°í™” (YOLO11n)
    // ğŸ”§ ìˆ˜ì •: Xcode ìë™ ìƒì„± í´ë˜ìŠ¤ ì‚¬ìš© (Bundle ê²½ë¡œ ë¬¸ì œ í•´ê²°)
    private func setupCoreMLDetector() {

        logInfo("YOLO11n CoreML ì´ˆê¸°í™” ì‹œì‘", category: "YOLO11n")
        logMemory("YOLO11n ë¡œë“œ ì „")

        do {
            // ğŸ”§ ìë™ ìƒì„±ëœ YOLO11nDetector í´ë˜ìŠ¤ ì‚¬ìš©
            let config = MLModelConfiguration()
            config.computeUnits = .all  // Neural Engine + GPU + CPU ìë™ ì„ íƒ

            let yolo = try YOLO11nDetector(configuration: config)
            yoloModel = try VNCoreMLModel(for: yolo.model)
            logInfo("Yolo11n ë¡œë“œ ì„±ê³µ")
            logMemory("YOLO11n ë¡œë“œ í›„")
        } catch {
            logError("YOLO11n ë¡œë“œ ì‹¤íŒ¨ - error: \(error.localizedDescription)", category: "YOLO11n")
            logDebug("YOLO11n ìƒì„¸ ì—ëŸ¬ - \(error)", category: "YOLO11n")
            yoloModel = nil
        }
    }

    // MARK: - ONNX Runtime ì´ˆê¸°í™” (RTMPoseë§Œ)
    private func setupONNXRuntime() {
        logInfo("ONNX Runtime ì´ˆê¸°í™” ì‹œì‘", category: "RTMPose")

        do {
            // 1. Environment ìƒì„±
            env = try ORTEnv(loggingLevel: ORTLoggingLevel.warning)
            logDebug("Environment ìƒì„± ì„±ê³µ", category: "RTMPose")

            // 2. RTMPoseìš© Session Options (CoreML GPU ê°€ì†)
            let poseOptions = try ORTSessionOptions()

            // CoreML Execution Provider í™œì„±í™” (GPU ê°€ì†)
            do {
                try poseOptions.appendCoreMLExecutionProvider()
                logInfo("RTMPose CoreML GPU ê°€ì† í™œì„±í™”", category: "RTMPose")
            } catch {
                logWarning("RTMPose CoreML í™œì„±í™” ì‹¤íŒ¨, CPU í´ë°± - error: \(error.localizedDescription)", category: "RTMPose")
            }

            // ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • (ìµœëŒ€ ì„±ëŠ¥)
            try poseOptions.setIntraOpNumThreads(6)
            try poseOptions.setGraphOptimizationLevel(.all)

            // 3. RTMPose ì„¸ì…˜ ìƒì„±
            logMemory("RTMPose ë¡œë“œ ì „")

            logDebug("RTMPose ëª¨ë¸ ë¡œë”© ì¤‘ - path: \(poseModelPath)", category: "RTMPose")
            poseSession = try ORTSession(env: env!, modelPath: poseModelPath, sessionOptions: poseOptions)
            logInfo("RTMPose ë¡œë“œ ì„±ê³µ - accelerator: CoreML GPU", category: "RTMPose")
            logMemory("RTMPose ë¡œë“œ í›„")

            logInfo("ONNX Runtime ì´ˆê¸°í™” ì™„ë£Œ", category: "RTMPose")

        } catch {
            logError("ONNX Runtime ì´ˆê¸°í™” ì‹¤íŒ¨ - error: \(error.localizedDescription)", category: "RTMPose")
            logDebug("RTMPose ìƒì„¸ ì—ëŸ¬ - \(error)", category: "RTMPose")
            env = nil
            poseSession = nil
        }
    }

    // MARK: - YOLO11n CoreMLë¡œ ì‚¬ëŒ ê²€ì¶œ (BBoxë§Œ í•„ìš”í•  ë•Œ)
    func detectPersonBBox(from image: UIImage) -> CGRect? {
        guard let yoloModel = yoloModel else {
            logError("YOLO11n ì¶”ë¡  ì‹¤íŒ¨ - error: ëª¨ë¸ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ", category: "YOLO11n")
            return nil
        }

        return detectPersonWithCoreML(from: image, model: yoloModel)
    }

    // MARK: - YOLO11n CoreMLë¡œ ëª¨ë“  ì‚¬ëŒ ê²€ì¶œ (ë©€í‹° person)
    func detectAllPersonBBoxes(from image: UIImage) -> [CGRect] {
        guard let yoloModel = yoloModel else {
            logError("YOLO11n ì¶”ë¡  ì‹¤íŒ¨ - error: ëª¨ë¸ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ", category: "YOLO11n")
            return []
        }

        return detectAllPersonsWithCoreML(from: image, model: yoloModel)
    }

    // MARK: - ì„¸ì…˜ ìƒíƒœ í™•ì¸
    var isReady: Bool {
        return yoloModel != nil && poseSession != nil && env != nil
    }

    // MARK: - í¬ì¦ˆ ì¶”ì •
    func detectPose(from image: UIImage) -> RTMPoseResult? {
        guard let yoloModel = yoloModel,
              let poseSession = poseSession,
              let env = env else {
            logError("RTMPose ì¶”ë¡  ì‹¤íŒ¨ - error: ì„¸ì…˜ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ", category: "RTMPose")
            return nil
        }

        // 1. YOLO11n CoreMLë¡œ ì‚¬ëŒ ê²€ì¶œ
        guard let detectedBox = detectPersonWithCoreML(from: image, model: yoloModel) else {
            logWarning("YOLO11n ì‚¬ëŒ ê²€ì¶œ ì•ˆë¨ - í¬ì¦ˆ ì¶”ì • ê±´ë„ˆëœ€", category: "YOLO11n")
            return nil
        }

        logDebug("YOLO11n ì‚¬ëŒ ê²€ì¶œ ì„±ê³µ - boundingBox: \(detectedBox)", category: "YOLO11n")

        // 2. ê²€ì¶œëœ ì˜ì—­ìœ¼ë¡œ í¬ì¦ˆ ì¶”ì •
        let keypoints = estimatePose(from: image, boundingBox: detectedBox, using: poseSession, env: env)

        if let keypoints = keypoints {
            logDebug("RTMPose ì¶”ë¡  ì„±ê³µ - keypoints: \(keypoints.count)", category: "RTMPose")
        } else {
            logError("RTMPose ì¶”ë¡  ì‹¤íŒ¨", category: "RTMPose")
        }

        return keypoints.map { RTMPoseResult(keypoints: $0, boundingBox: detectedBox) }
    }

    // MARK: - YOLO11n CoreML ì‚¬ëŒ ê²€ì¶œ (ë‹¨ì¼)
    private static let visionQueue = DispatchQueue(label: "yolo11n.vision", qos: .userInitiated)

    private func detectPersonWithCoreML(from image: UIImage, model: VNCoreMLModel) -> CGRect? {
        guard let cgImage = image.cgImage else { return nil }

        var resultBox: CGRect?
        let semaphore = DispatchSemaphore(value: 0)
        let imageWidth = cgImage.width
        let imageHeight = cgImage.height

        // ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ Vision ìš”ì²­ ì‹¤í–‰
        Self.visionQueue.async {
            // ğŸ”¥ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€: Vision ìš”ì²­ ê°ì²´ ì¦‰ì‹œ í•´ì œ
            autoreleasepool {
                let request = VNCoreMLRequest(model: model) { request, error in
                    if let error = error {
                        logError("YOLO11n ì¶”ë¡  ì‹¤íŒ¨ - error: \(error.localizedDescription)", category: "YOLO11n")
                        semaphore.signal()
                        return
                    }

                    // VNRecognizedObjectObservationìœ¼ë¡œ ê²°ê³¼ íŒŒì‹±
                    guard let results = request.results as? [VNRecognizedObjectObservation] else {
                        logWarning("YOLO11n ê²°ê³¼ í˜•ì‹ ë¶ˆì¼ì¹˜", category: "YOLO11n")
                        semaphore.signal()
                        return
                    }

                    // person í´ë˜ìŠ¤ë§Œ í•„í„°ë§í•˜ê³  ê°€ì¥ ë†’ì€ confidence ì„ íƒ
                    var bestBox: CGRect?
                    var bestConfidence: Float = 0.3  // ìµœì†Œ ì„ê³„ê°’

                    for observation in results {
                        // person í´ë˜ìŠ¤ í™•ì¸ (COCO í´ë˜ìŠ¤ 0)
                        if let topLabel = observation.labels.first,
                           topLabel.identifier == "person" || topLabel.identifier == "0",
                           topLabel.confidence > bestConfidence {
                            bestConfidence = topLabel.confidence
                            // Vision ì¢Œí‘œê³„ (ì¢Œí•˜ë‹¨ ì›ì ) â†’ UIKit ì¢Œí‘œê³„ ë³€í™˜
                            let bbox = observation.boundingBox
                            bestBox = CGRect(
                                x: bbox.minX * CGFloat(imageWidth),
                                y: (1 - bbox.maxY) * CGFloat(imageHeight),
                                width: bbox.width * CGFloat(imageWidth),
                                height: bbox.height * CGFloat(imageHeight)
                            )
                        }
                    }

                    resultBox = bestBox
                    semaphore.signal()
                }

                request.imageCropAndScaleOption = .scaleFill

                let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
                do {
                    try handler.perform([request])
                } catch {
                    logError("YOLO11n Vision ì‹¤í–‰ ì‹¤íŒ¨ - error: \(error.localizedDescription)", category: "YOLO11n")
                    semaphore.signal()
                }
            }
        }

        semaphore.wait()
        return resultBox
    }

    // MARK: - YOLO11n CoreML ëª¨ë“  ì‚¬ëŒ ê²€ì¶œ (ë©€í‹°)
    private func detectAllPersonsWithCoreML(from image: UIImage, model: VNCoreMLModel) -> [CGRect] {
        guard let cgImage = image.cgImage else { return [] }

        var resultBoxes: [CGRect] = []
        let semaphore = DispatchSemaphore(value: 0)
        let imageWidth = cgImage.width
        let imageHeight = cgImage.height

        // ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ Vision ìš”ì²­ ì‹¤í–‰
        Self.visionQueue.async {
            // ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€: Vision ìš”ì²­ ê°ì²´ ì¦‰ì‹œ í•´ì œ
            autoreleasepool {
                let request = VNCoreMLRequest(model: model) { request, error in
                    if let error = error {
                        logError("YOLO11n ì¶”ë¡  ì‹¤íŒ¨ - error: \(error.localizedDescription)", category: "YOLO11n")
                        semaphore.signal()
                        return
                    }

                    guard let results = request.results as? [VNRecognizedObjectObservation] else {
                        semaphore.signal()
                        return
                    }

                    let threshold: Float = 0.3

                    for observation in results {
                        if let topLabel = observation.labels.first,
                           topLabel.identifier == "person" || topLabel.identifier == "0",
                           topLabel.confidence > threshold {
                            let bbox = observation.boundingBox
                            let box = CGRect(
                                x: bbox.minX * CGFloat(imageWidth),
                                y: (1 - bbox.maxY) * CGFloat(imageHeight),
                                width: bbox.width * CGFloat(imageWidth),
                                height: bbox.height * CGFloat(imageHeight)
                            )
                            resultBoxes.append(box)
                        }
                    }
                    semaphore.signal()
                }

                request.imageCropAndScaleOption = .scaleFill

                let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
                do {
                    try handler.perform([request])
                } catch {
                    logError("YOLO11n Vision ì‹¤í–‰ ì‹¤íŒ¨ - error: \(error.localizedDescription)", category: "YOLO11n")
                    semaphore.signal()
                }
            }
        }

        semaphore.wait()
        return resultBoxes
    }

    // MARK: - RTMPose í¬ì¦ˆ ì¶”ì •
    private func estimatePose(from image: UIImage, boundingBox: CGRect, using session: ORTSession, env: ORTEnv) -> [(point: CGPoint, confidence: Float)]? {
        guard let cgImage = image.cgImage else { return nil }

        // ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ í¬ë¡­
        guard let croppedImage = cropImage(cgImage, rect: boundingBox) else { return nil }

        // 192x256ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        let inputSize = poseInputSize
        guard let resizedImage = resizeImage(croppedImage, targetSize: inputSize) else { return nil }

        // ì´ë¯¸ì§€ë¥¼ Float ë°°ì—´ë¡œ ë³€í™˜
        let pixelData = preprocessImage(resizedImage, size: inputSize)

        do {
            // ì…ë ¥ í…ì„œ ìƒì„± - [1, 3, 256, 192]
            let inputShape: [NSNumber] = [1, 3, NSNumber(value: Int(inputSize.height)), NSNumber(value: Int(inputSize.width))]
            let inputTensor = try ORTValue(
                tensorData: NSMutableData(data: pixelData),
                elementType: .float,
                shape: inputShape
            )

            // ì¶”ë¡  ì‹¤í–‰
            let outputs = try session.run(
                withInputs: ["input": inputTensor],
                outputNames: ["simcc_x", "simcc_y"],
                runOptions: nil
            )

            guard let simccX = outputs["simcc_x"],
                  let simccY = outputs["simcc_y"] else {
                logError("RTMPose ì¶”ë¡  ì‹¤íŒ¨ - error: SimCC ì¶œë ¥ ì—†ìŒ", category: "RTMPose")
                return nil
            }

            // SimCC ì¶œë ¥ íŒŒì‹±í•˜ì—¬ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (133ê°œ)
            let imageSize = image.size
            return parseRTMPoseSimCCOutput(simccX: simccX, simccY: simccY, boundingBox: boundingBox, imageSize: imageSize)

        } catch {
            logError("RTMPose ì¶”ë¡  ì‹¤íŒ¨ - error: \(error.localizedDescription)", category: "RTMPose")
            logDebug("RTMPose ìƒì„¸ ì—ëŸ¬ - \(error)", category: "RTMPose")
            return nil
        }
    }

    // MARK: - ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜ë“¤
    private func resizeImage(_ cgImage: CGImage, targetSize: CGSize) -> CGImage? {
        let width = Int(targetSize.width)
        let height = Int(targetSize.height)

        guard let context = CGContext(
            data: nil,
            width: width,
            height: height,
            bitsPerComponent: 8,
            bytesPerRow: width * 4,
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipLast.rawValue
        ) else { return nil }

        context.interpolationQuality = .high
        context.draw(cgImage, in: CGRect(origin: .zero, size: targetSize))
        return context.makeImage()
    }

    private func cropImage(_ cgImage: CGImage, rect: CGRect) -> CGImage? {
        // ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì¶©ë¶„íˆ í™•ì¥ (ì†ì´ í¬í•¨ë˜ë„ë¡ íŒ¨ë”© ì¦ê°€)
        // ğŸ”¥ ì† ì¸ì‹ ê°œì„ : íŒ¨ë”©ì„ 0.2ì—ì„œ 0.4ë¡œ ì¦ê°€
        let padding: CGFloat = 0.4  // 40% íŒ¨ë”©ìœ¼ë¡œ ì†ê¹Œì§€ í¬í•¨
        let expandedRect = CGRect(
            x: rect.minX - rect.width * padding,
            y: rect.minY - rect.height * padding,
            width: rect.width * (1 + 2 * padding),
            height: rect.height * (1 + 2 * padding)
        ).intersection(CGRect(x: 0, y: 0, width: cgImage.width, height: cgImage.height))

        return cgImage.cropping(to: expandedRect)
    }

    // ğŸ”¥ Accelerate ê¸°ë°˜ ê³ ì† ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    private func preprocessImage(_ cgImage: CGImage, size: CGSize) -> Data {
        let width = Int(size.width)
        let height = Int(size.height)
        let pixelCount = width * height

        var rawData = [UInt8](repeating: 0, count: pixelCount * 4)

        guard let context = CGContext(
            data: &rawData,
            width: width,
            height: height,
            bitsPerComponent: 8,
            bytesPerRow: width * 4,
            space: CGColorSpaceCreateDeviceRGB(),
            bitmapInfo: CGImageAlphaInfo.noneSkipLast.rawValue
        ) else {
            return Data()
        }

        context.draw(cgImage, in: CGRect(x: 0, y: 0, width: width, height: height))

        // ğŸ”¥ vDSPë¥¼ ì‚¬ìš©í•œ ë²¡í„°í™”ëœ ì •ê·œí™” (ImageNet í‰ê· /í‘œì¤€í¸ì°¨)
        var floatData = [Float](repeating: 0, count: pixelCount * 3)
        let mean: [Float] = [0.485, 0.456, 0.406]
        let std: [Float] = [0.229, 0.224, 0.225]

        // ê° ì±„ë„ë³„ ì²˜ë¦¬ (ë³‘ë ¬í™”)
        DispatchQueue.concurrentPerform(iterations: 3) { c in
            var channelData = [Float](repeating: 0, count: pixelCount)

            // RGBAì—ì„œ í•´ë‹¹ ì±„ë„ ì¶”ì¶œ (strideë¡œ ì ‘ê·¼)
            for i in 0..<pixelCount {
                channelData[i] = Float(rawData[i * 4 + c])
            }

            // vDSP: /255.0 ì •ê·œí™”
            var scale: Float = 1.0 / 255.0
            vDSP_vsmul(channelData, 1, &scale, &channelData, 1, vDSP_Length(pixelCount))

            // vDSP: (x - mean) ë¹¼ê¸°
            var negMean = -mean[c]
            vDSP_vsadd(channelData, 1, &negMean, &channelData, 1, vDSP_Length(pixelCount))

            // vDSP: / std ë‚˜ëˆ„ê¸°
            var invStd = 1.0 / std[c]
            vDSP_vsmul(channelData, 1, &invStd, &channelData, 1, vDSP_Length(pixelCount))

            // CHW í¬ë§·ìœ¼ë¡œ ë³µì‚¬
            let offset = c * pixelCount
            for i in 0..<pixelCount {
                floatData[offset + i] = channelData[i]
            }
        }

        return Data(bytes: &floatData, count: floatData.count * MemoryLayout<Float>.size)
    }

    // MARK: - RTMPose SimCC ì¶œë ¥ íŒŒì‹±
    private func parseRTMPoseSimCCOutput(simccX: ORTValue, simccY: ORTValue, boundingBox: CGRect, imageSize: CGSize) -> [(point: CGPoint, confidence: Float)]? {
        // SimCC ì¶œë ¥ í˜•ì‹:
        // simcc_x: [1, num_keypoints, 384] - x ì¢Œí‘œ í™•ë¥  ë¶„í¬
        // simcc_y: [1, num_keypoints, 512] - y ì¢Œí‘œ í™•ë¥  ë¶„í¬

        guard let xData = try? simccX.tensorData() as NSData,
              let yData = try? simccY.tensorData() as NSData else { return nil }
        guard let xShape = try? simccX.tensorTypeAndShapeInfo().shape,
              let yShape = try? simccY.tensorTypeAndShapeInfo().shape else { return nil }

        let numKeypoints = xShape[1].intValue
        let xBins = xShape[2].intValue  // 384
        let yBins = yShape[2].intValue  // 512

        if numKeypoints != 133 {
            logWarning("RTMPose í‚¤í¬ì¸íŠ¸ ìˆ˜ ë¶ˆì¼ì¹˜ - expected: 133 | actual: \(numKeypoints)", category: "RTMPose")
            return nil
        }

        var keypoints: [(point: CGPoint, confidence: Float)] = []
        let xPointer = xData.bytes.bindMemory(to: Float.self, capacity: xData.length / MemoryLayout<Float>.size)
        let yPointer = yData.bytes.bindMemory(to: Float.self, capacity: yData.length / MemoryLayout<Float>.size)

        for i in 0..<numKeypoints {
            // x ì¢Œí‘œ: argmax ì°¾ê¸°
            let xOffset = i * xBins
            var maxXIdx = 0
            var maxXVal: Float = -Float.infinity
            for j in 0..<xBins {
                let val = xPointer[xOffset + j]
                if val > maxXVal {
                    maxXVal = val
                    maxXIdx = j
                }
            }

            // y ì¢Œí‘œ: argmax ì°¾ê¸°
            let yOffset = i * yBins
            var maxYIdx = 0
            var maxYVal: Float = -Float.infinity
            for j in 0..<yBins {
                let val = yPointer[yOffset + j]
                if val > maxYVal {
                    maxYVal = val
                    maxYIdx = j
                }
            }

            // SimCC ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            // 384 bins -> 192 pixels, 512 bins -> 256 pixels (ê°ê° 2ë°° í•´ìƒë„)
            let xNorm = CGFloat(maxXIdx) / CGFloat(xBins) * poseInputSize.width
            let yNorm = CGFloat(maxYIdx) / CGFloat(yBins) * poseInputSize.height

            // ë°”ìš´ë”© ë°•ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜ í›„ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì •ê·œí™” (0.0~1.0)
            let point = CGPoint(
                x: (boundingBox.minX + (xNorm / poseInputSize.width) * boundingBox.width) / imageSize.width,
                y: (boundingBox.minY + (yNorm / poseInputSize.height) * boundingBox.height) / imageSize.height
            )

            // ì‹ ë¢°ë„: ë‘ í™•ë¥ ì˜ í‰ê· 
            let confidence = (maxXVal + maxYVal) / 2.0

            keypoints.append((point: point, confidence: confidence))

            // ì† í‚¤í¬ì¸íŠ¸ ë””ë²„ê·¸ (91-132ë²ˆ)
            if i >= 91 && i <= 132 {
                if confidence < 0.3 {
                    let handName = i <= 111 ? "ì™¼ì†" : "ì˜¤ë¥¸ì†"
                    let keypointIndex = i <= 111 ? i - 91 : i - 112
                    if keypointIndex % 5 == 0 {  // 5ê°œë§ˆë‹¤ í•œ ë²ˆë§Œ ë¡œê·¸
                        logDebug("RTMPose \(handName) í‚¤í¬ì¸íŠ¸ ì‹ ë¢°ë„ ë‚®ìŒ - index: \(keypointIndex) | confidence: \(String(format: "%.2f", confidence))", category: "RTMPose")
                    }
                }
            }
        }

        // ì† í‚¤í¬ì¸íŠ¸ ìš”ì•½ í†µê³„
        let leftHandConfidences = (91...111).compactMap { keypoints[$0].confidence }
        let rightHandConfidences = (112...132).compactMap { keypoints[$0].confidence }

        let leftHandAvg = leftHandConfidences.reduce(0, +) / Float(leftHandConfidences.count)
        let rightHandAvg = rightHandConfidences.reduce(0, +) / Float(rightHandConfidences.count)

        if leftHandAvg < 0.5 || rightHandAvg < 0.5 {
            logDebug("RTMPose ì† ì¸ì‹ í‰ê·  ì‹ ë¢°ë„ - leftHand: \(String(format: "%.2f", leftHandAvg)) | rightHand: \(String(format: "%.2f", rightHandAvg))", category: "RTMPose")
            if leftHandAvg < 0.3 || rightHandAvg < 0.3 {
                logWarning("RTMPose ì† ì¸ì‹ ì‹ ë¢°ë„ ë§¤ìš° ë‚®ìŒ - ì†ì´ í”„ë ˆì„ ë°–ì´ê±°ë‚˜ ê°€ë ¤ì§", category: "RTMPose")
            }
        }

        return keypoints
    }
}
